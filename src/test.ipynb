{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/envs/cse447/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.nn import LSTM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"uonlp/CulturaX\", \"en\", split='train', streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_head = ds.take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(example):\n",
    "    text = example[\"text\"]\n",
    "    pairs = []\n",
    "    for i in range(1, len(text)):\n",
    "        inputs = text[:i]\n",
    "        outputs = text[i]\n",
    "        if len(inputs) > MAX_LENGTH:\n",
    "            inputs = inputs[-MAX_LENGTH:]\n",
    "        pairs.append({\"input\": inputs, \"output\": outputs})\n",
    "    return pairs\n",
    "data_pairs = []\n",
    "for d in ds_head:\n",
    "    data_pairs.extend(create_pairs(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "1864434\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "num_chars = 0\n",
    "for d in ds_head:\n",
    "    num_chars += len(d['text'])\n",
    "    chars.update(d['text'])\n",
    "print(len(chars))\n",
    "print(num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    return [char_to_idx[char] for char in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "outputs = []\n",
    "for d in data_pairs:\n",
    "    sequences.append(torch.tensor(encode_sequence(d[\"input\"]), dtype=torch.long))\n",
    "    outputs.append(torch.tensor(char_to_idx[d[\"output\"]], dtype=torch.long))\n",
    "\n",
    "X = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "y = torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1863934, 50])\n",
      "tensor([39, 50, 55,  3, 36, 81,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "torch.Size([1863934])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[5])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'X': X, 'y': y,\n",
    "    'char_to_idx': char_to_idx, 'idx_to_char': idx_to_char\n",
    "}, 'dataset_splits.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1863934, 50])\n",
      "torch.Size([1863934])\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '#': 6, '$': 7, '%': 8, '&': 9, \"'\": 10, '(': 11, ')': 12, '*': 13, '+': 14, ',': 15, '-': 16, '.': 17, '/': 18, '0': 19, '1': 20, '2': 21, '3': 22, '4': 23, '5': 24, '6': 25, '7': 26, '8': 27, '9': 28, ':': 29, ';': 30, '<': 31, '=': 32, '>': 33, '?': 34, '@': 35, 'A': 36, 'B': 37, 'C': 38, 'D': 39, 'E': 40, 'F': 41, 'G': 42, 'H': 43, 'I': 44, 'J': 45, 'K': 46, 'L': 47, 'M': 48, 'N': 49, 'O': 50, 'P': 51, 'Q': 52, 'R': 53, 'S': 54, 'T': 55, 'U': 56, 'V': 57, 'W': 58, 'X': 59, 'Y': 60, 'Z': 61, '[': 62, '\\\\': 63, ']': 64, '^': 65, '_': 66, '`': 67, 'a': 68, 'b': 69, 'c': 70, 'd': 71, 'e': 72, 'f': 73, 'g': 74, 'h': 75, 'i': 76, 'j': 77, 'k': 78, 'l': 79, 'm': 80, 'n': 81, 'o': 82, 'p': 83, 'q': 84, 'r': 85, 's': 86, 't': 87, 'u': 88, 'v': 89, 'w': 90, 'x': 91, 'y': 92, 'z': 93, '{': 94, '|': 95, '}': 96, '~': 97, '\\x9d': 98, '\\xa0': 99, 'Â£': 100, 'Â§': 101, 'Â©': 102, 'Â«': 103, 'Â®': 104, 'Â°': 105, 'Â±': 106, 'Â²': 107, 'Â´': 108, 'Â·': 109, 'Âº': 110, 'Â»': 111, 'Â¼': 112, 'Â½': 113, 'Ã‡': 114, 'Ã“': 115, 'Ã—': 116, 'Ã ': 117, 'Ã¡': 118, 'Ã¢': 119, 'Ã£': 120, 'Ã¤': 121, 'Ã¦': 122, 'Ã§': 123, 'Ã¨': 124, 'Ã©': 125, 'Ãª': 126, 'Ã«': 127, 'Ã¬': 128, 'Ã­': 129, 'Ã®': 130, 'Ã¯': 131, 'Ã°': 132, 'Ã±': 133, 'Ã³': 134, 'Ã´': 135, 'Ãµ': 136, 'Ã¶': 137, 'Ã¸': 138, 'Ã¹': 139, 'Ãº': 140, 'Ã¼': 141, 'Ä': 142, 'Äƒ': 143, 'ÄŒ': 144, 'Ä': 145, 'Ä': 146, 'Ä‘': 147, 'Ä': 148, 'ÄŸ': 149, 'Ä£': 150, 'Ä±': 151, 'Å™': 152, 'Å': 153, 'ÅŸ': 154, 'Å¡': 155, 'Å¾': 156, 'Æ°': 157, 'Ê»': 158, 'Î”': 159, 'Î•': 160, 'Î¬': 161, 'Î·': 162, 'Î¹': 163, 'Îº': 164, 'Î»': 165, 'Î¼': 166, 'Î½': 167, 'Ğœ': 168, 'Ğ ': 169, 'Ğ¡': 170, 'Ğ¤': 171, 'Ğ°': 172, 'Ğ²': 173, 'Ğ´': 174, 'Ğµ': 175, 'Ğ¸': 176, 'Ğº': 177, 'Ğ»': 178, 'Ğ¼': 179, 'Ğ½': 180, 'Ğ¾': 181, 'Ğ¿': 182, 'Ñ€': 183, 'Ñ': 184, 'Ñ‚': 185, 'Ñ‡': 186, 'Ñ': 187, 'Ñ‘': 188, 'Ø§': 189, 'Ø¨': 190, 'Ø©': 191, 'Ø±': 192, 'Ø¹': 193, 'Ù„': 194, 'ÙŠ': 195, 'áº¡': 196, 'áº£': 197, 'áº¥': 198, 'áº§': 199, 'áº­': 200, 'áº¯': 201, 'áº½': 202, 'áº¿': 203, 'á»': 204, 'á»ƒ': 205, 'á»‹': 206, 'á»': 207, 'á»•': 208, 'á»›': 209, 'á»': 210, 'á»£': 211, 'á»§': 212, 'á»©': 213, 'á»­': 214, 'á»±': 215, 'á»³': 216, '\\u2009': 217, '\\u200b': 218, '\\u200e': 219, 'â€': 220, 'â€“': 221, 'â€”': 222, 'â€˜': 223, 'â€™': 224, 'â€œ': 225, 'â€': 226, 'â€¢': 227, 'â€¦': 228, 'â€³': 229, 'â€º': 230, 'â‚¬': 231, 'â‚±': 232, 'â„¢': 233, 'â†': 234, 'â†‘': 235, 'â†’': 236, 'âˆ’': 237, 'â‘ ': 238, 'â‘¡': 239, 'â‘¢': 240, 'â‘£': 241, 'â‘¤': 242, 'â”': 243, 'â•«': 244, 'â–€': 245, 'â– ': 246, 'â–·': 247, 'â˜…': 248, 'âœ…': 249, 'âŸ¨': 250, 'âŸ©': 251, 'ã€': 252, 'ã€‘': 253, 'ä¸€': 254, 'ä¸­': 255, 'ä½“': 256, 'ä½œ': 257, 'åŠ›': 258, 'å‘': 259, 'å¬': 260, 'å›': 261, 'å¸ƒ': 262, 'æ…¢': 263, 'æ‘˜': 264, 'æ–‡': 265, 'æ—¥': 266, 'æœŸ': 267, 'æœ¬': 268, 'æ´’': 269, 'æ½‡': 270, 'ç®€': 271, 'è€…': 272, 'è‹±': 273, 'è¦': 274, 'èª': 275, 'è¯­': 276, 'èµ°': 277, 'é€Ÿ': 278, '\\uf04a': 279, 'ï¼š': 280, 'ï¿½': 281, 'ğ´': 282, 'ğ‘…': 283, 'ğŸ‡¦': 284, 'ğŸ‡¨': 285, 'ğŸ¶': 286, 'ğŸ‘': 287, 'ğŸ˜†': 288, 'ğŸ˜‰': 289, 'ğŸ˜Š': 290, 'ğŸ˜•': 291, 'ğŸ™‚': 292, 'ğŸ¤—': 293}\n",
      "{1: '\\t', 2: '\\n', 3: ' ', 4: '!', 5: '\"', 6: '#', 7: '$', 8: '%', 9: '&', 10: \"'\", 11: '(', 12: ')', 13: '*', 14: '+', 15: ',', 16: '-', 17: '.', 18: '/', 19: '0', 20: '1', 21: '2', 22: '3', 23: '4', 24: '5', 25: '6', 26: '7', 27: '8', 28: '9', 29: ':', 30: ';', 31: '<', 32: '=', 33: '>', 34: '?', 35: '@', 36: 'A', 37: 'B', 38: 'C', 39: 'D', 40: 'E', 41: 'F', 42: 'G', 43: 'H', 44: 'I', 45: 'J', 46: 'K', 47: 'L', 48: 'M', 49: 'N', 50: 'O', 51: 'P', 52: 'Q', 53: 'R', 54: 'S', 55: 'T', 56: 'U', 57: 'V', 58: 'W', 59: 'X', 60: 'Y', 61: 'Z', 62: '[', 63: '\\\\', 64: ']', 65: '^', 66: '_', 67: '`', 68: 'a', 69: 'b', 70: 'c', 71: 'd', 72: 'e', 73: 'f', 74: 'g', 75: 'h', 76: 'i', 77: 'j', 78: 'k', 79: 'l', 80: 'm', 81: 'n', 82: 'o', 83: 'p', 84: 'q', 85: 'r', 86: 's', 87: 't', 88: 'u', 89: 'v', 90: 'w', 91: 'x', 92: 'y', 93: 'z', 94: '{', 95: '|', 96: '}', 97: '~', 98: '\\x9d', 99: '\\xa0', 100: 'Â£', 101: 'Â§', 102: 'Â©', 103: 'Â«', 104: 'Â®', 105: 'Â°', 106: 'Â±', 107: 'Â²', 108: 'Â´', 109: 'Â·', 110: 'Âº', 111: 'Â»', 112: 'Â¼', 113: 'Â½', 114: 'Ã‡', 115: 'Ã“', 116: 'Ã—', 117: 'Ã ', 118: 'Ã¡', 119: 'Ã¢', 120: 'Ã£', 121: 'Ã¤', 122: 'Ã¦', 123: 'Ã§', 124: 'Ã¨', 125: 'Ã©', 126: 'Ãª', 127: 'Ã«', 128: 'Ã¬', 129: 'Ã­', 130: 'Ã®', 131: 'Ã¯', 132: 'Ã°', 133: 'Ã±', 134: 'Ã³', 135: 'Ã´', 136: 'Ãµ', 137: 'Ã¶', 138: 'Ã¸', 139: 'Ã¹', 140: 'Ãº', 141: 'Ã¼', 142: 'Ä', 143: 'Äƒ', 144: 'ÄŒ', 145: 'Ä', 146: 'Ä', 147: 'Ä‘', 148: 'Ä', 149: 'ÄŸ', 150: 'Ä£', 151: 'Ä±', 152: 'Å™', 153: 'Å', 154: 'ÅŸ', 155: 'Å¡', 156: 'Å¾', 157: 'Æ°', 158: 'Ê»', 159: 'Î”', 160: 'Î•', 161: 'Î¬', 162: 'Î·', 163: 'Î¹', 164: 'Îº', 165: 'Î»', 166: 'Î¼', 167: 'Î½', 168: 'Ğœ', 169: 'Ğ ', 170: 'Ğ¡', 171: 'Ğ¤', 172: 'Ğ°', 173: 'Ğ²', 174: 'Ğ´', 175: 'Ğµ', 176: 'Ğ¸', 177: 'Ğº', 178: 'Ğ»', 179: 'Ğ¼', 180: 'Ğ½', 181: 'Ğ¾', 182: 'Ğ¿', 183: 'Ñ€', 184: 'Ñ', 185: 'Ñ‚', 186: 'Ñ‡', 187: 'Ñ', 188: 'Ñ‘', 189: 'Ø§', 190: 'Ø¨', 191: 'Ø©', 192: 'Ø±', 193: 'Ø¹', 194: 'Ù„', 195: 'ÙŠ', 196: 'áº¡', 197: 'áº£', 198: 'áº¥', 199: 'áº§', 200: 'áº­', 201: 'áº¯', 202: 'áº½', 203: 'áº¿', 204: 'á»', 205: 'á»ƒ', 206: 'á»‹', 207: 'á»', 208: 'á»•', 209: 'á»›', 210: 'á»', 211: 'á»£', 212: 'á»§', 213: 'á»©', 214: 'á»­', 215: 'á»±', 216: 'á»³', 217: '\\u2009', 218: '\\u200b', 219: '\\u200e', 220: 'â€', 221: 'â€“', 222: 'â€”', 223: 'â€˜', 224: 'â€™', 225: 'â€œ', 226: 'â€', 227: 'â€¢', 228: 'â€¦', 229: 'â€³', 230: 'â€º', 231: 'â‚¬', 232: 'â‚±', 233: 'â„¢', 234: 'â†', 235: 'â†‘', 236: 'â†’', 237: 'âˆ’', 238: 'â‘ ', 239: 'â‘¡', 240: 'â‘¢', 241: 'â‘£', 242: 'â‘¤', 243: 'â”', 244: 'â•«', 245: 'â–€', 246: 'â– ', 247: 'â–·', 248: 'â˜…', 249: 'âœ…', 250: 'âŸ¨', 251: 'âŸ©', 252: 'ã€', 253: 'ã€‘', 254: 'ä¸€', 255: 'ä¸­', 256: 'ä½“', 257: 'ä½œ', 258: 'åŠ›', 259: 'å‘', 260: 'å¬', 261: 'å›', 262: 'å¸ƒ', 263: 'æ…¢', 264: 'æ‘˜', 265: 'æ–‡', 266: 'æ—¥', 267: 'æœŸ', 268: 'æœ¬', 269: 'æ´’', 270: 'æ½‡', 271: 'ç®€', 272: 'è€…', 273: 'è‹±', 274: 'è¦', 275: 'èª', 276: 'è¯­', 277: 'èµ°', 278: 'é€Ÿ', 279: '\\uf04a', 280: 'ï¼š', 281: 'ï¿½', 282: 'ğ´', 283: 'ğ‘…', 284: 'ğŸ‡¦', 285: 'ğŸ‡¨', 286: 'ğŸ¶', 287: 'ğŸ‘', 288: 'ğŸ˜†', 289: 'ğŸ˜‰', 290: 'ğŸ˜Š', 291: 'ğŸ˜•', 292: 'ğŸ™‚', 293: 'ğŸ¤—'}\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('dataset_splits.pth')\n",
    "X, y = data['X'], data['y']\n",
    "char_to_idx = data['char_to_idx']\n",
    "idx_to_char = data['idx_to_char']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, outputs, device):\n",
    "        self.sequences = sequences.to(device)\n",
    "        self.outputs = outputs.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.outputs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharDataset(train_set[:][0], train_set[:][1], device)\n",
    "val_dataset = CharDataset(val_set[:][0], val_set[:][1], device)\n",
    "test_dataset = CharDataset(test_set[:][0], test_set[:][1], device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Output size is vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last LSTM output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.5096, Train Acc: 0.3184, Val Loss: 2.1357, Val Acc: 0.3986\n",
      "Epoch 2: Train Loss: 1.9921, Train Acc: 0.4370, Val Loss: 1.8841, Val Acc: 0.4662\n",
      "Epoch 3: Train Loss: 1.8026, Train Acc: 0.4891, Val Loss: 1.7489, Val Acc: 0.5037\n",
      "Epoch 4: Train Loss: 1.6870, Train Acc: 0.5196, Val Loss: 1.6685, Val Acc: 0.5251\n",
      "Epoch 5: Train Loss: 1.6094, Train Acc: 0.5391, Val Loss: 1.6179, Val Acc: 0.5385\n",
      "Epoch 6: Train Loss: 1.5525, Train Acc: 0.5531, Val Loss: 1.5846, Val Acc: 0.5467\n",
      "Epoch 7: Train Loss: 1.5071, Train Acc: 0.5646, Val Loss: 1.5628, Val Acc: 0.5524\n",
      "Epoch 8: Train Loss: 1.4687, Train Acc: 0.5745, Val Loss: 1.5479, Val Acc: 0.5569\n",
      "Epoch 9: Train Loss: 1.4346, Train Acc: 0.5832, Val Loss: 1.5386, Val Acc: 0.5597\n",
      "Epoch 10: Train Loss: 1.4035, Train Acc: 0.5912, Val Loss: 1.5339, Val Acc: 0.5614\n",
      "Epoch 11: Train Loss: 1.3745, Train Acc: 0.5989, Val Loss: 1.5321, Val Acc: 0.5629\n",
      "Epoch 12: Train Loss: 1.3470, Train Acc: 0.6063, Val Loss: 1.5326, Val Acc: 0.5634\n",
      "Epoch 13: Train Loss: 1.3209, Train Acc: 0.6135, Val Loss: 1.5367, Val Acc: 0.5637\n",
      "Epoch 14: Train Loss: 1.2956, Train Acc: 0.6209, Val Loss: 1.5421, Val Acc: 0.5634\n",
      "Epoch 15: Train Loss: 1.2892, Train Acc: 0.6216, Val Loss: 1.5205, Val Acc: 0.5690\n",
      "Epoch 16: Train Loss: nan, Train Acc: 0.0779, Val Loss: nan, Val Acc: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define search space\n",
    "lr = 1e-4\n",
    "embed_dim = 12\n",
    "hidden_dim = 256\n",
    "num_layers = 4\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(char_to_idx) + 1\n",
    "model = CharLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_correct, train_total, running_loss = 0, 0, 0\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "        train_total += targets.numel()\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct, val_total, val_loss = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.numel()\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss\n",
    "    }\n",
    "    torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=4, lr=0.0005\n",
    "# Epoch 1: Train Loss: 2.6495, Train Acc: 0.2909, Val Loss: 2.2317, Val Acc: 0.3762\n",
    "# Epoch 2: Train Loss: 2.1025, Train Acc: 0.4079, Val Loss: 2.0086, Val Acc: 0.4310\n",
    "# Epoch 3: Train Loss: 1.9364, Train Acc: 0.4512, Val Loss: 1.8976, Val Acc: 0.4602\n",
    "# Epoch 4: Train Loss: 1.8342, Train Acc: 0.4787, Val Loss: 1.8251, Val Acc: 0.4817\n",
    "# Epoch 5: Train Loss: 1.7652, Train Acc: 0.4972, Val Loss: 1.7746, Val Acc: 0.4959\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=4, lr=0.0001\n",
    "# Epoch 1: Train Loss: 3.0272, Train Acc: 0.2024, Val Loss: 2.6667, Val Acc: 0.2684\n",
    "# Epoch 2: Train Loss: 2.4985, Train Acc: 0.3109, Val Loss: 2.3811, Val Acc: 0.3391\n",
    "# Epoch 3: Train Loss: 2.2985, Train Acc: 0.3593, Val Loss: 2.2286, Val Acc: 0.3745\n",
    "# Epoch 4: Train Loss: 2.1686, Train Acc: 0.3902, Val Loss: 2.1256, Val Acc: 0.4000\n",
    "# Epoch 5: Train Loss: 2.0770, Train Acc: 0.4131, Val Loss: 2.0493, Val Acc: 0.4199\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=6, lr=0.0005\n",
    "# Epoch 1: Train Loss: 3.2215, Train Acc: 0.1623, Val Loss: 3.2171, Val Acc: 0.1615\n",
    "# Epoch 2: Train Loss: 3.2188, Train Acc: 0.1623, Val Loss: 3.2173, Val Acc: 0.1615\n",
    "# Epoch 3: Train Loss: 3.2188, Train Acc: 0.1623, Val Loss: 3.2174, Val Acc: 0.1615\n",
    "# Epoch 4: Train Loss: 3.2187, Train Acc: 0.1623, Val Loss: 3.2173, Val Acc: 0.1615\n",
    "# Epoch 5: Train Loss: 3.2187, Train Acc: 0.1623, Val Loss: 3.2173, Val Acc: 0.1615\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=6, lr=0.0001\n",
    "# Epoch 1: Train Loss: 3.1103, Train Acc: 0.1844, Val Loss: 2.7249, Val Acc: 0.2656\n",
    "# Epoch 2: Train Loss: 2.5756, Train Acc: 0.2931, Val Loss: 2.4823, Val Acc: 0.3131\n",
    "# Epoch 3: Train Loss: 2.4225, Train Acc: 0.3272, Val Loss: 2.3761, Val Acc: 0.3368\n",
    "# Epoch 4: Train Loss: 2.3367, Train Acc: 0.3435, Val Loss: 2.3057, Val Acc: 0.3497\n",
    "# Epoch 5: Train Loss: 2.2668, Train Acc: 0.3586, Val Loss: 2.2428, Val Acc: 0.3663\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=256, num_layers=4, lr=0.0005\n",
    "# Epoch 1: Train Loss: 2.6740, Train Acc: 0.2834, Val Loss: 2.1641, Val Acc: 0.3929\n",
    "# Epoch 2: Train Loss: 2.0015, Train Acc: 0.4329, Val Loss: 1.8835, Val Acc: 0.4626\n",
    "# Epoch 3: Train Loss: 1.7981, Train Acc: 0.4868, Val Loss: 1.7642, Val Acc: 0.4960\n",
    "# Epoch 4: Train Loss: 1.6872, Train Acc: 0.5156, Val Loss: 1.7059, Val Acc: 0.5112\n",
    "# Epoch 5: Train Loss: 1.6139, Train Acc: 0.5344, Val Loss: 1.6718, Val Acc: 0.5219\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=256, num_layers=4, lr=0.0001\n",
    "# Epoch 1: Train Loss: 2.9654, Train Acc: 0.2245, Val Loss: 2.6269, Val Acc: 0.2990\n",
    "# Epoch 2: Train Loss: 2.5003, Train Acc: 0.3223, Val Loss: 2.3906, Val Acc: 0.3427\n",
    "# Epoch 3: Train Loss: 2.3068, Train Acc: 0.3618, Val Loss: 2.2278, Val Acc: 0.3792\n",
    "# Epoch 4: Train Loss: 2.1653, Train Acc: 0.3951, Val Loss: 2.1111, Val Acc: 0.4095\n",
    "# Epoch 5: Train Loss: 2.0632, Train Acc: 0.4199, Val Loss: 2.0294, Val Acc: 0.4275\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=256, num_layers=6, lr=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7174\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for inputs, targets in test_dataloader:\n",
    "    outputs = model(inputs)\n",
    "    top3_preds = torch.topk(outputs, 3, dim=1)[1]\n",
    "    translated_output = set([idx_to_char[str(idx.item())] for idx in top3_preds[0]])\n",
    "    translated_target = idx_to_char[str(targets[0].item())]\n",
    "    if translated_target in translated_output:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    # print(\"Inputs:\", ''.join(translated_input))\n",
    "    # print(\"Predicted:\", translated_output)\n",
    "    # print(\"Targets:\", translated_target)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '\\t', '2': '\\n', '3': ' ', '4': '!', '5': '\"', '6': '#', '7': '$', '8': '%', '9': '&', '10': \"'\", '11': '(', '12': ')', '13': '*', '14': '+', '15': ',', '16': '-', '17': '.', '18': '/', '19': '0', '20': '1', '21': '2', '22': '3', '23': '4', '24': '5', '25': '6', '26': '7', '27': '8', '28': '9', '29': ':', '30': ';', '31': '<', '32': '=', '33': '>', '34': '?', '35': '@', '36': 'A', '37': 'B', '38': 'C', '39': 'D', '40': 'E', '41': 'F', '42': 'G', '43': 'H', '44': 'I', '45': 'J', '46': 'K', '47': 'L', '48': 'M', '49': 'N', '50': 'O', '51': 'P', '52': 'Q', '53': 'R', '54': 'S', '55': 'T', '56': 'U', '57': 'V', '58': 'W', '59': 'X', '60': 'Y', '61': 'Z', '62': '[', '63': '\\\\', '64': ']', '65': '^', '66': '_', '67': '`', '68': 'a', '69': 'b', '70': 'c', '71': 'd', '72': 'e', '73': 'f', '74': 'g', '75': 'h', '76': 'i', '77': 'j', '78': 'k', '79': 'l', '80': 'm', '81': 'n', '82': 'o', '83': 'p', '84': 'q', '85': 'r', '86': 's', '87': 't', '88': 'u', '89': 'v', '90': 'w', '91': 'x', '92': 'y', '93': 'z', '94': '{', '95': '|', '96': '}', '97': '~', '98': '\\x9d', '99': '\\xa0', '100': 'Â£', '101': 'Â§', '102': 'Â©', '103': 'Â«', '104': '\\xad', '105': 'Â®', '106': 'Â°', '107': 'Â±', '108': 'Â²', '109': 'Â´', '110': 'Âµ', '111': 'Â·', '112': 'Âº', '113': 'Â»', '114': 'Â¼', '115': 'Â½', '116': 'Â¾', '117': 'Ã', '118': 'Ã‚', '119': 'Ã‡', '120': 'Ã“', '121': 'Ã–', '122': 'Ã—', '123': 'Ã ', '124': 'Ã¡', '125': 'Ã¢', '126': 'Ã£', '127': 'Ã¤', '128': 'Ã¦', '129': 'Ã§', '130': 'Ã¨', '131': 'Ã©', '132': 'Ãª', '133': 'Ã«', '134': 'Ã¬', '135': 'Ã­', '136': 'Ã®', '137': 'Ã¯', '138': 'Ã°', '139': 'Ã±', '140': 'Ã²', '141': 'Ã³', '142': 'Ã´', '143': 'Ãµ', '144': 'Ã¶', '145': 'Ã¸', '146': 'Ã¹', '147': 'Ãº', '148': 'Ã¼', '149': 'Ã½', '150': 'Ä', '151': 'Äƒ', '152': 'ÄŒ', '153': 'Ä', '154': 'Ä', '155': 'Ä‘', '156': 'Ä›', '157': 'Ä', '158': 'ÄŸ', '159': 'Ä£', '160': 'Ä©', '161': 'Ä±', '162': 'Å™', '163': 'Å›', '164': 'Å', '165': 'ÅŸ', '166': 'Å¡', '167': 'Å¾', '168': 'Æ¡', '169': 'Æ°', '170': 'Ê»', '171': 'Î”', '172': 'Î•', '173': 'Î¬', '174': 'Î±', '175': 'Î²', '176': 'Î·', '177': 'Î¹', '178': 'Îº', '179': 'Î»', '180': 'Î¼', '181': 'Î½', '182': 'Ï‡', '183': 'Ğ‘', '184': 'Ğ–', '185': 'Ğœ', '186': 'ĞŸ', '187': 'Ğ ', '188': 'Ğ¡', '189': 'Ğ¤', '190': 'Ğ°', '191': 'Ğ²', '192': 'Ğ´', '193': 'Ğµ', '194': 'Ğ¸', '195': 'Ğ¹', '196': 'Ğº', '197': 'Ğ»', '198': 'Ğ¼', '199': 'Ğ½', '200': 'Ğ¾', '201': 'Ğ¿', '202': 'Ñ€', '203': 'Ñ', '204': 'Ñ‚', '205': 'Ñ‡', '206': 'Ñˆ', '207': 'ÑŒ', '208': 'Ñ', '209': 'Ñ‘', '210': 'Ø§', '211': 'Ø¨', '212': 'Ø©', '213': 'Ø±', '214': 'Ø¹', '215': 'Ù„', '216': 'ÙŠ', '217': 'à¸„', '218': 'à¸•', '219': 'à¸—', '220': 'à¸š', '221': 'à¸›', '222': 'à¸¢', '223': 'à¸£', '224': 'à¸¨', '225': 'à¸ª', '226': 'à¸°', '227': 'à¸²', '228': 'à¸¹', '229': 'à¹€', '230': 'à¹„', '231': 'à¹ˆ', '232': 'áº¡', '233': 'áº£', '234': 'áº¥', '235': 'áº§', '236': 'áº«', '237': 'áº­', '238': 'áº¯', '239': 'áº±', '240': 'áº³', '241': 'áº·', '242': 'áº¹', '243': 'áº½', '244': 'áº¿', '245': 'á»', '246': 'á»ƒ', '247': 'á»…', '248': 'á»‡', '249': 'á»‰', '250': 'á»‹', '251': 'á»', '252': 'á»', '253': 'á»‘', '254': 'á»“', '255': 'á»•', '256': 'á»—', '257': 'á»™', '258': 'á»›', '259': 'á»', '260': 'á»Ÿ', '261': 'á»£', '262': 'á»¥', '263': 'á»§', '264': 'á»©', '265': 'á»«', '266': 'á»­', '267': 'á»¯', '268': 'á»±', '269': 'á»³', '270': '\\u2008', '271': '\\u2009', '272': '\\u200b', '273': '\\u200e', '274': 'â€', '275': 'â€“', '276': 'â€”', '277': 'â€˜', '278': 'â€™', '279': 'â€œ', '280': 'â€', '281': 'â€', '282': 'â€ ', '283': 'â€¢', '284': 'â€¦', '285': 'â€²', '286': 'â€³', '287': 'â€º', '288': 'â‚¬', '289': 'â‚±', '290': 'â‚¹', '291': 'â„—', '292': 'â„˜', '293': 'â„¢', '294': 'â†', '295': 'â†‘', '296': 'â†’', '297': 'â†“', '298': 'â‡', '299': 'â‡’', '300': 'âˆ’', '301': 'â‰ˆ', '302': 'â‰¤', '303': 'â‰¥', '304': 'â‘ ', '305': 'â‘¡', '306': 'â‘¢', '307': 'â‘£', '308': 'â‘¤', '309': 'â”', '310': 'â•«', '311': 'â–€', '312': 'â– ', '313': 'â–·', '314': 'â–º', '315': 'â—¦', '316': 'â˜…', '317': 'â˜´', '318': 'â™•', '319': 'âœ…', '320': 'âŸ¨', '321': 'âŸ©', '322': 'ã€', '323': 'ã€‘', '324': 'ä¸€', '325': 'ä¸­', '326': 'ä½“', '327': 'ä½œ', '328': 'åŠ›', '329': 'å‘', '330': 'å¬', '331': 'å›', '332': 'å¸ƒ', '333': 'æ…¢', '334': 'æ‘˜', '335': 'æ–‡', '336': 'æ–°', '337': 'æ— ', '338': 'æ—¥', '339': 'æœ€', '340': 'æœŸ', '341': 'æœ¬', '342': 'æ´’', '343': 'æ¸…', '344': 'æ½‡', '345': 'ç ', '346': 'ç®€', '347': 'ç¹', '348': 'è€…', '349': 'è‹±', '350': 'è¦', '351': 'èª', '352': 'è¯­', '353': 'èµ°', '354': 'é€Ÿ', '355': 'é“', '356': 'é«”', '357': 'é«˜', '358': 'êµ­', '359': 'ë…¸', '360': 'ë¼', '361': 'ë°”', '362': 'ì‚¬', '363': 'ì´', '364': 'ì§€', '365': 'ì¹´', '366': 'íŠ¸', '367': 'í•œ', '368': '\\uf04a', '369': 'ï¼š', '370': 'ï¿½', '371': 'ğ´', '372': 'ğ‘…', '373': 'ğŸ‡¦', '374': 'ğŸ‡¨', '375': 'ğŸ¶', '376': 'ğŸ‘', '377': 'ğŸ˜€', '378': 'ğŸ˜', '379': 'ğŸ˜†', '380': 'ğŸ˜‰', '381': 'ğŸ˜Š', '382': 'ğŸ˜•', '383': 'ğŸ˜¥', '384': 'ğŸ™‚', '385': 'ğŸ¤”', '386': 'ğŸ¤—', '387': 'ğŸ¤£'}\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse447",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
