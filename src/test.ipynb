{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/envs/cse447/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.nn import LSTM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bad split: test. Available splits: ['train']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muonlp/CulturaX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m ds_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muonlp/CulturaX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ds_val \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muonlp/CulturaX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/datasets/load.py:2148\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n\u001b[0;32m-> 2148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_streaming_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m   2151\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   2152\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2153\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2156\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2157\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/datasets/builder.py:1271\u001b[0m, in \u001b[0;36mDatasetBuilder.as_streaming_dataset\u001b[0;34m(self, split, base_path)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     splits_generator \u001b[38;5;241m=\u001b[39m splits_generators[split]\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Available splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(splits_generators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m datasets \u001b[38;5;241m=\u001b[39m map_nested(\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_streaming_dataset_single,\n\u001b[1;32m   1276\u001b[0m     splits_generator,\n\u001b[1;32m   1277\u001b[0m     map_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1278\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Bad split: test. Available splits: ['train']"
     ]
    }
   ],
   "source": [
    "ds_train = load_dataset(\"uonlp/CulturaX\", \"en\", split='train', streaming=True)\n",
    "ds_test = load_dataset(\"uonlp/CulturaX\", \"en\", split='test', streaming=True)\n",
    "ds_val = load_dataset(\"uonlp/CulturaX\", \"en\", split='validation', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'DOT Announces 2008 Exploration Program - Redorbit\\nCALGARY, ALBERTA--(Marketwire - July 31, 2008) - DOT Resources Ltd. (TSX VENTURE:DOT) (\"DOT\" or the \"Corporation\") is pleased to announce it will be completing a deep 3D Induced Polarization/ Resistivity (\"IP\") survey on its Dot porphyry copper property (the \"Property\") located 17 kilometres south of the Highland Valley Mining complex, in central British Columbia. The purpose of the 3D IP survey is to test the depth extension of the copper-molybdenum mineralization outlined to date on the Southeast zone and to explore the area of low magnetic susceptibility for zones of sulphide mineralization.\\nThe total field magnetometer survey completed over the Property in 2007 located a large area measuring 3 kilometres (\"km\") by 1 km of low magnetic susceptibility (see Figure 1) which is interpreted to reflect either:\\n- alteration, or\\n- the intersection of cross-cutting structures, or\\n- a combination of both features\\nThe porphyry deposits located to date in the Highland Valley District all exhibit a combination of the above stated features. The zones of copper mineralization located to date on the property (see News Release dated October 11, 2007) cover a strike length of at least 1,000 metres (\"m\") (see Figure 1) and are located on the northeast border of the large area of low magnetic susceptibility. These zones are characterized by widespread copper and more restricted molybdenum mineralization and occur in a broad north- northwest trending shear zone that could represent either:\\n- a \"leakage\" zone, or\\n- a mineralized satellite zone from a deeper mineralized body\\nThe distribution of the copper and molybdenum mineralization in these zones suggests the possibility of two separate mineralizing events.\\nThe objectives of the 2008 exploration program are to continue the exploration of the targets identified in 2007 and to test the strike and depth extension of the copper-molybdenum mineralization in the Southeast zone. The first phase of the 2008 exploration program is expected to commence in late August and consists of a deep (approximately 200 m below surface) 3D IP survey over the Southeast zone of copper-molybdenum mineralization and the large area of low magnetic susceptibility.\\nContingent on the results of the first phase of the IP survey, a second phase of additional IP surveying and diamond drill testing is planned. Diamond drill testing to the southeast and the depth extension of the Southeast zone is also planned commencing in the third quarter 2008.\\nImportant factors that could cause actual results to differ materially from DOT\\'s expectations include fluctuations in commodity prices and currency exchange rates; uncertainties relating to interpretation of drill results and the geology, continuity and grade of mineral deposits, the possibility of adverse developments in the financial markets generally, and other risks and uncertainties disclosed under the heading \"Caution Regarding Forward- Looking Statements\" and in other information released by DOT and filed with the appropriate regulatory agencies.\\nTo view Figure 1 - DOT Property - Total Field Magnetometer Map, please visit the following link: http://media3.marketwire.com/docs/ dmap.pdf.', 'timestamp': '2018/04/21 21:46:33', 'url': 'http://www.redorbit.com/news/business/1503000/dot_announces_2008_exploration_program/', 'source': 'mC4'}\n"
     ]
    }
   ],
   "source": [
    "ds_head = ds.take(150)\n",
    "for d in ds_head:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(example):\n",
    "    text = example[\"text\"]\n",
    "    pairs = []\n",
    "    for i in range(1, len(text)):\n",
    "        inputs = text[:i]\n",
    "        outputs = text[i]\n",
    "        if len(inputs) > 100:\n",
    "            inputs = inputs[-100:]\n",
    "        pairs.append({\"input\": inputs, \"output\": outputs})\n",
    "    return pairs\n",
    "data_pairs = []\n",
    "for d in ds_head:\n",
    "    data_pairs.extend(create_pairs(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "438048\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "num_chars = 0\n",
    "for d in ds_head:\n",
    "    num_chars += len(d['text'])\n",
    "    chars.update(d['text'])\n",
    "print(len(chars))\n",
    "print(num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    return [char_to_idx[char] for char in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '#': 6, '$': 7, '%': 8, '&': 9, \"'\": 10, '(': 11, ')': 12, '*': 13, '+': 14, ',': 15, '-': 16, '.': 17, '/': 18, '0': 19, '1': 20, '2': 21, '3': 22, '4': 23, '5': 24, '6': 25, '7': 26, '8': 27, '9': 28, ':': 29, ';': 30, '<': 31, '=': 32, '>': 33, '?': 34, '@': 35, 'A': 36, 'B': 37, 'C': 38, 'D': 39, 'E': 40, 'F': 41, 'G': 42, 'H': 43, 'I': 44, 'J': 45, 'K': 46, 'L': 47, 'M': 48, 'N': 49, 'O': 50, 'P': 51, 'Q': 52, 'R': 53, 'S': 54, 'T': 55, 'U': 56, 'V': 57, 'W': 58, 'X': 59, 'Y': 60, 'Z': 61, '[': 62, '\\\\': 63, ']': 64, '^': 65, '_': 66, '`': 67, 'a': 68, 'b': 69, 'c': 70, 'd': 71, 'e': 72, 'f': 73, 'g': 74, 'h': 75, 'i': 76, 'j': 77, 'k': 78, 'l': 79, 'm': 80, 'n': 81, 'o': 82, 'p': 83, 'q': 84, 'r': 85, 's': 86, 't': 87, 'u': 88, 'v': 89, 'w': 90, 'x': 91, 'y': 92, 'z': 93, '{': 94, '|': 95, '}': 96, '~': 97, '\\x9d': 98, '¬£': 99, '¬ß': 100, '¬©': 101, '¬´': 102, '¬Æ': 103, '¬∑': 104, '¬∫': 105, '¬ª': 106, '√†': 107, '√°': 108, '√¢': 109, '√£': 110, '√¶': 111, '√©': 112, '√¨': 113, '√≠': 114, '√∞': 115, '√±': 116, '√≥': 117, '√¥': 118, '√µ': 119, '√∂': 120, '√π': 121, '√∫': 122, 'ƒÉ': 123, 'ƒë': 124, '∆∞': 125, 'Œî': 126, '–ú': 127, '–†': 128, '–°': 129, '–§': 130, '–∞': 131, '–≤': 132, '–¥': 133, '–µ': 134, '–∏': 135, '–∫': 136, '–º': 137, '–Ω': 138, '–æ': 139, '—Ä': 140, '—Å': 141, '—á': 142, '—ë': 143, '·∫°': 144, '·∫£': 145, '·∫•': 146, '·∫ß': 147, '·∫≠': 148, '·∫Ω': 149, '·∫ø': 150, '·ªÅ': 151, '·ªÉ': 152, '·ªã': 153, '·ªç': 154, '·ªï': 155, '·ªõ': 156, '·ªù': 157, '·ª£': 158, '·ªß': 159, '·ª©': 160, '·ª≠': 161, '·ª±': 162, '·ª≥': 163, '\\u200b': 164, '‚Äì': 165, '‚Äî': 166, '‚Äò': 167, '‚Äô': 168, '‚Äú': 169, '‚Äù': 170, '‚Ä¢': 171, '‚Ä¶': 172, '‚Ä∫': 173, '‚Ç¨': 174, '‚Ç±': 175, '‚Üí': 176, '‚îê': 177, '‚ï´': 178, '‚ñÄ': 179, '‚ñ†': 180, 'ÔøΩ': 181, 'üëç': 182, 'üòÜ': 183, 'üòâ': 184, 'üòä': 185, 'üòï': 186, 'ü§ó': 187}\n",
      "{1: '\\t', 2: '\\n', 3: ' ', 4: '!', 5: '\"', 6: '#', 7: '$', 8: '%', 9: '&', 10: \"'\", 11: '(', 12: ')', 13: '*', 14: '+', 15: ',', 16: '-', 17: '.', 18: '/', 19: '0', 20: '1', 21: '2', 22: '3', 23: '4', 24: '5', 25: '6', 26: '7', 27: '8', 28: '9', 29: ':', 30: ';', 31: '<', 32: '=', 33: '>', 34: '?', 35: '@', 36: 'A', 37: 'B', 38: 'C', 39: 'D', 40: 'E', 41: 'F', 42: 'G', 43: 'H', 44: 'I', 45: 'J', 46: 'K', 47: 'L', 48: 'M', 49: 'N', 50: 'O', 51: 'P', 52: 'Q', 53: 'R', 54: 'S', 55: 'T', 56: 'U', 57: 'V', 58: 'W', 59: 'X', 60: 'Y', 61: 'Z', 62: '[', 63: '\\\\', 64: ']', 65: '^', 66: '_', 67: '`', 68: 'a', 69: 'b', 70: 'c', 71: 'd', 72: 'e', 73: 'f', 74: 'g', 75: 'h', 76: 'i', 77: 'j', 78: 'k', 79: 'l', 80: 'm', 81: 'n', 82: 'o', 83: 'p', 84: 'q', 85: 'r', 86: 's', 87: 't', 88: 'u', 89: 'v', 90: 'w', 91: 'x', 92: 'y', 93: 'z', 94: '{', 95: '|', 96: '}', 97: '~', 98: '\\x9d', 99: '¬£', 100: '¬ß', 101: '¬©', 102: '¬´', 103: '¬Æ', 104: '¬∑', 105: '¬∫', 106: '¬ª', 107: '√†', 108: '√°', 109: '√¢', 110: '√£', 111: '√¶', 112: '√©', 113: '√¨', 114: '√≠', 115: '√∞', 116: '√±', 117: '√≥', 118: '√¥', 119: '√µ', 120: '√∂', 121: '√π', 122: '√∫', 123: 'ƒÉ', 124: 'ƒë', 125: '∆∞', 126: 'Œî', 127: '–ú', 128: '–†', 129: '–°', 130: '–§', 131: '–∞', 132: '–≤', 133: '–¥', 134: '–µ', 135: '–∏', 136: '–∫', 137: '–º', 138: '–Ω', 139: '–æ', 140: '—Ä', 141: '—Å', 142: '—á', 143: '—ë', 144: '·∫°', 145: '·∫£', 146: '·∫•', 147: '·∫ß', 148: '·∫≠', 149: '·∫Ω', 150: '·∫ø', 151: '·ªÅ', 152: '·ªÉ', 153: '·ªã', 154: '·ªç', 155: '·ªï', 156: '·ªõ', 157: '·ªù', 158: '·ª£', 159: '·ªß', 160: '·ª©', 161: '·ª≠', 162: '·ª±', 163: '·ª≥', 164: '\\u200b', 165: '‚Äì', 166: '‚Äî', 167: '‚Äò', 168: '‚Äô', 169: '‚Äú', 170: '‚Äù', 171: '‚Ä¢', 172: '‚Ä¶', 173: '‚Ä∫', 174: '‚Ç¨', 175: '‚Ç±', 176: '‚Üí', 177: '‚îê', 178: '‚ï´', 179: '‚ñÄ', 180: '‚ñ†', 181: 'ÔøΩ', 182: 'üëç', 183: 'üòÜ', 184: 'üòâ', 185: 'üòä', 186: 'üòï', 187: 'ü§ó'}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "outputs = []\n",
    "for d in data_pairs:\n",
    "    sequences.append(torch.tensor(encode_sequence(d[\"input\"]), dtype=torch.long))\n",
    "    outputs.append(torch.tensor(char_to_idx[d[\"output\"]], dtype=torch.long))\n",
    "X_train = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "y_train = torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([437948, 100])\n",
      "tensor([39,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([437948])\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, outputs):\n",
    "        self.sequences = sequences\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        output = self.outputs[idx]\n",
    "        return sequence, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CharDataset(X_train, y_train)\n",
    "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "torch.save({'X': X_train, 'y': y_train}, \"./train_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"./train_dataset.pt\")\n",
    "train_dataset = CharDataset(data['X'], data['y'])\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Output size is vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last LSTM output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.5071\n",
      "Loss: 3.5341\n",
      "Loss: 3.1997\n",
      "Loss: 3.4474\n",
      "Loss: 3.2438\n",
      "Loss: 3.1500\n",
      "Loss: 3.3261\n",
      "Loss: 2.9506\n",
      "Loss: 4.1606\n",
      "Loss: 3.2293\n",
      "Loss: 3.1494\n",
      "Loss: 3.1633\n",
      "Loss: 3.4746\n",
      "Loss: 3.1419\n",
      "Loss: 3.1628\n",
      "Loss: 3.3034\n",
      "Loss: 2.7172\n",
      "Loss: 3.3559\n",
      "Loss: 3.3383\n",
      "Loss: 3.2977\n",
      "Loss: 3.2969\n",
      "Loss: 3.0898\n",
      "Loss: 3.1281\n",
      "Loss: 3.2703\n",
      "Loss: 3.1861\n",
      "Loss: 3.1769\n",
      "Loss: 2.9191\n",
      "Loss: 3.0554\n",
      "Loss: 3.4426\n",
      "Loss: 3.3386\n",
      "Loss: 3.1336\n",
      "Loss: 3.4490\n",
      "Loss: 3.2310\n",
      "Loss: 3.3383\n",
      "Loss: 3.2543\n",
      "Loss: 2.9732\n",
      "Loss: 3.0182\n",
      "Loss: 3.1868\n",
      "Loss: 3.1471\n",
      "Loss: 3.0621\n",
      "Loss: 3.4064\n",
      "Loss: 3.2404\n",
      "Loss: 2.9424\n",
      "Loss: 3.1476\n",
      "Loss: 3.4871\n",
      "Loss: 2.9830\n",
      "Loss: 3.5914\n",
      "Loss: 3.4563\n",
      "Loss: 3.1582\n",
      "Loss: 3.0937\n",
      "Loss: 3.2334\n",
      "Loss: 3.5465\n",
      "Loss: 3.3893\n",
      "Loss: 3.4133\n",
      "Loss: 3.6386\n",
      "Loss: 3.4696\n",
      "Loss: 3.4246\n",
      "Loss: 3.2072\n",
      "Loss: 3.1458\n",
      "Loss: 3.1667\n",
      "Loss: 2.8759\n",
      "Loss: 2.9735\n",
      "Loss: 2.9470\n",
      "Loss: 3.2661\n",
      "Loss: 3.2276\n",
      "Loss: 3.3414\n",
      "Loss: 2.8227\n",
      "Loss: 2.9724\n",
      "Loss: 3.3293\n",
      "Loss: 3.1119\n",
      "Loss: 2.8708\n",
      "Loss: 3.1238\n",
      "Loss: 2.5376\n",
      "Loss: 2.9613\n",
      "Loss: 3.1822\n",
      "Loss: 3.4488\n",
      "Loss: 3.0990\n",
      "Loss: 3.1583\n",
      "Loss: 3.0031\n",
      "Loss: 3.1937\n",
      "Loss: 3.4980\n",
      "Loss: 3.1781\n",
      "Loss: 2.8515\n",
      "Loss: 2.7299\n",
      "Loss: 3.2553\n",
      "Loss: 3.0361\n",
      "Loss: 2.9302\n",
      "Loss: 2.9326\n",
      "Loss: 2.7389\n",
      "Loss: 2.7024\n",
      "Loss: 3.0275\n",
      "Loss: 2.5794\n",
      "Loss: 3.0101\n",
      "Loss: 2.8150\n",
      "Loss: 2.7765\n",
      "Loss: 3.0758\n",
      "Loss: 2.4027\n",
      "Loss: 3.1589\n",
      "Loss: 3.0682\n",
      "Loss: 2.8431\n",
      "Loss: 3.0585\n",
      "Loss: 3.0273\n",
      "Loss: 3.2824\n",
      "Loss: 2.9206\n",
      "Loss: 3.0201\n",
      "Loss: 2.8251\n",
      "Loss: 3.0463\n",
      "Loss: 2.6131\n",
      "Loss: 2.9125\n",
      "Loss: 2.3527\n",
      "Loss: 2.7031\n",
      "Loss: 2.9411\n",
      "Loss: 3.6863\n",
      "Loss: 2.8589\n",
      "Loss: 2.7675\n",
      "Loss: 2.9029\n",
      "Loss: 2.9457\n",
      "Loss: 3.0157\n",
      "Loss: 3.1870\n",
      "Loss: 2.7650\n",
      "Loss: 2.8145\n",
      "Loss: 2.8017\n",
      "Loss: 2.9082\n",
      "Loss: 2.9224\n",
      "Loss: 2.8056\n",
      "Loss: 2.9544\n",
      "Loss: 2.5823\n",
      "Loss: 3.0495\n",
      "Loss: 2.9754\n",
      "Loss: 2.8880\n",
      "Loss: 2.7421\n",
      "Loss: 2.6326\n",
      "Loss: 3.2591\n",
      "Loss: 2.7753\n",
      "Loss: 3.0066\n",
      "Loss: 2.7734\n",
      "Epoch 1, Loss: 2.6010\n",
      "Loss: 3.1623\n",
      "Loss: 2.8875\n",
      "Loss: 2.7140\n",
      "Loss: 2.6932\n",
      "Loss: 2.6279\n",
      "Loss: 2.6707\n",
      "Loss: 2.5792\n",
      "Loss: 2.9500\n",
      "Loss: 2.5591\n",
      "Loss: 2.7256\n",
      "Loss: 2.7287\n",
      "Loss: 2.8076\n",
      "Loss: 2.3065\n",
      "Loss: 2.9014\n",
      "Loss: 3.6293\n",
      "Loss: 2.8940\n",
      "Loss: 2.2277\n",
      "Loss: 2.8125\n",
      "Loss: 2.8861\n",
      "Loss: 2.7350\n",
      "Loss: 2.9338\n",
      "Loss: 2.5433\n",
      "Loss: 2.7975\n",
      "Loss: 3.1767\n",
      "Loss: 2.9942\n",
      "Loss: 2.6155\n",
      "Loss: 3.0523\n",
      "Loss: 2.8595\n",
      "Loss: 2.6844\n",
      "Loss: 2.8853\n",
      "Loss: 3.0226\n",
      "Loss: 2.4914\n",
      "Loss: 2.8599\n",
      "Loss: 3.3096\n",
      "Loss: 2.5886\n",
      "Loss: 2.6970\n",
      "Loss: 3.0699\n",
      "Loss: 3.2051\n",
      "Loss: 2.7726\n",
      "Loss: 2.6018\n",
      "Loss: 3.1608\n",
      "Loss: 2.8320\n",
      "Loss: 2.5892\n",
      "Loss: 2.6092\n",
      "Loss: 2.4319\n",
      "Loss: 2.9355\n",
      "Loss: 3.1126\n",
      "Loss: 2.6072\n",
      "Loss: 2.3151\n",
      "Loss: 2.8269\n",
      "Loss: 2.9550\n",
      "Loss: 2.7146\n",
      "Loss: 2.7786\n",
      "Loss: 2.8558\n",
      "Loss: 2.5349\n",
      "Loss: 3.0581\n",
      "Loss: 3.1831\n",
      "Loss: 2.5363\n",
      "Loss: 2.9234\n",
      "Loss: 2.5228\n",
      "Loss: 3.1947\n",
      "Loss: 2.9314\n",
      "Loss: 2.9363\n",
      "Loss: 2.7158\n",
      "Loss: 2.7243\n",
      "Loss: 2.3210\n",
      "Loss: 2.5807\n",
      "Loss: 2.6018\n",
      "Loss: 2.7395\n",
      "Loss: 2.3544\n",
      "Loss: 2.8327\n",
      "Loss: 2.5887\n",
      "Loss: 2.4050\n",
      "Loss: 2.4178\n",
      "Loss: 2.6228\n",
      "Loss: 2.8321\n",
      "Loss: 2.8611\n",
      "Loss: 2.4590\n",
      "Loss: 2.2580\n",
      "Loss: 2.6030\n",
      "Loss: 2.2236\n",
      "Loss: 2.5447\n",
      "Loss: 2.8299\n",
      "Loss: 2.6599\n",
      "Loss: 2.6783\n",
      "Loss: 3.1894\n",
      "Loss: 2.4524\n",
      "Loss: 2.8474\n",
      "Loss: 2.3403\n",
      "Loss: 3.0669\n",
      "Loss: 2.9244\n",
      "Loss: 2.8020\n",
      "Loss: 2.9073\n",
      "Loss: 2.5474\n",
      "Loss: 2.4052\n",
      "Loss: 2.7365\n",
      "Loss: 2.5528\n",
      "Loss: 2.9781\n",
      "Loss: 2.7979\n",
      "Loss: 2.9124\n",
      "Loss: 2.7804\n",
      "Loss: 3.0365\n",
      "Loss: 2.6797\n",
      "Loss: 2.9233\n",
      "Loss: 2.5845\n",
      "Loss: 2.6528\n",
      "Loss: 2.8292\n",
      "Loss: 2.4977\n",
      "Loss: 2.4016\n",
      "Loss: 2.5597\n",
      "Loss: 2.4740\n",
      "Loss: 2.6296\n",
      "Loss: 3.0411\n",
      "Loss: 2.4813\n",
      "Loss: 3.0391\n",
      "Loss: 2.4393\n",
      "Loss: 2.2619\n",
      "Loss: 2.5486\n",
      "Loss: 2.6457\n",
      "Loss: 2.0663\n",
      "Loss: 2.7664\n",
      "Loss: 2.4778\n",
      "Loss: 2.4306\n",
      "Loss: 2.7259\n",
      "Loss: 2.5699\n",
      "Loss: 2.8383\n",
      "Loss: 2.8066\n",
      "Loss: 2.6543\n",
      "Loss: 2.6726\n",
      "Loss: 2.8099\n",
      "Loss: 2.5135\n",
      "Loss: 2.7760\n",
      "Loss: 2.6383\n",
      "Loss: 2.4225\n",
      "Loss: 2.5464\n",
      "Loss: 2.3340\n",
      "Loss: 2.5689\n",
      "Epoch 2, Loss: 2.5942\n",
      "Loss: 2.8007\n",
      "Loss: 2.8477\n",
      "Loss: 2.4373\n",
      "Loss: 2.5510\n",
      "Loss: 2.6269\n",
      "Loss: 2.6091\n",
      "Loss: 2.5847\n",
      "Loss: 2.9062\n",
      "Loss: 3.1013\n",
      "Loss: 2.0457\n",
      "Loss: 1.9488\n",
      "Loss: 2.3300\n",
      "Loss: 2.5251\n",
      "Loss: 1.9121\n",
      "Loss: 2.8050\n",
      "Loss: 2.5373\n",
      "Loss: 3.1064\n",
      "Loss: 2.9119\n",
      "Loss: 3.4170\n",
      "Loss: 2.3171\n",
      "Loss: 2.4459\n",
      "Loss: 2.8159\n",
      "Loss: 2.1974\n",
      "Loss: 2.2378\n",
      "Loss: 2.5558\n",
      "Loss: 2.7128\n",
      "Loss: 3.1549\n",
      "Loss: 2.3995\n",
      "Loss: 2.7458\n",
      "Loss: 2.8440\n",
      "Loss: 2.1993\n",
      "Loss: 2.3588\n",
      "Loss: 2.3297\n",
      "Loss: 2.2840\n",
      "Loss: 2.4367\n",
      "Loss: 2.6119\n",
      "Loss: 2.9494\n",
      "Loss: 2.8016\n",
      "Loss: 2.1910\n",
      "Loss: 2.6247\n",
      "Loss: 2.5434\n",
      "Loss: 2.3299\n",
      "Loss: 2.3082\n",
      "Loss: 2.3007\n",
      "Loss: 2.9290\n",
      "Loss: 2.4067\n",
      "Loss: 2.3359\n",
      "Loss: 2.5595\n",
      "Loss: 2.2452\n",
      "Loss: 2.4132\n",
      "Loss: 2.5890\n",
      "Loss: 2.5992\n",
      "Loss: 2.7570\n",
      "Loss: 2.4160\n",
      "Loss: 1.9073\n",
      "Loss: 2.6981\n",
      "Loss: 2.7229\n",
      "Loss: 2.5814\n",
      "Loss: 2.3411\n",
      "Loss: 2.6369\n",
      "Loss: 2.4974\n",
      "Loss: 2.8556\n",
      "Loss: 1.7914\n",
      "Loss: 2.6268\n",
      "Loss: 2.7658\n",
      "Loss: 2.2290\n",
      "Loss: 2.4446\n",
      "Loss: 2.4620\n",
      "Loss: 2.8247\n",
      "Loss: 3.0009\n",
      "Loss: 2.3811\n",
      "Loss: 3.0450\n",
      "Loss: 2.5451\n",
      "Loss: 2.4905\n",
      "Loss: 2.4279\n",
      "Loss: 3.0518\n",
      "Loss: 2.9004\n",
      "Loss: 2.2380\n",
      "Loss: 2.0503\n",
      "Loss: 2.6079\n",
      "Loss: 2.0041\n",
      "Loss: 2.5060\n",
      "Loss: 2.0500\n",
      "Loss: 1.9918\n",
      "Loss: 2.0068\n",
      "Loss: 2.3108\n",
      "Loss: 2.4887\n",
      "Loss: 2.6143\n",
      "Loss: 2.2857\n",
      "Loss: 2.5341\n",
      "Loss: 2.6485\n",
      "Loss: 2.2473\n",
      "Loss: 2.7433\n",
      "Loss: 2.5884\n",
      "Loss: 2.5831\n",
      "Loss: 1.7315\n",
      "Loss: 2.2263\n",
      "Loss: 2.3474\n",
      "Loss: 2.1840\n",
      "Loss: 2.4035\n",
      "Loss: 2.2554\n",
      "Loss: 2.4565\n",
      "Loss: 3.0389\n",
      "Loss: 2.9187\n",
      "Loss: 2.7683\n",
      "Loss: 2.1221\n",
      "Loss: 2.3982\n",
      "Loss: 2.4495\n",
      "Loss: 2.9061\n",
      "Loss: 1.8671\n",
      "Loss: 2.2167\n",
      "Loss: 2.8095\n",
      "Loss: 2.3693\n",
      "Loss: 2.9398\n",
      "Loss: 2.5666\n",
      "Loss: 2.0458\n",
      "Loss: 2.4469\n",
      "Loss: 2.5179\n",
      "Loss: 1.8571\n",
      "Loss: 1.8910\n",
      "Loss: 1.9229\n",
      "Loss: 2.1846\n",
      "Loss: 2.3947\n",
      "Loss: 2.4282\n",
      "Loss: 2.8767\n",
      "Loss: 2.3684\n",
      "Loss: 2.1778\n",
      "Loss: 2.2661\n",
      "Loss: 2.4730\n",
      "Loss: 2.5777\n",
      "Loss: 2.3394\n",
      "Loss: 2.6503\n",
      "Loss: 2.1288\n",
      "Loss: 2.2720\n",
      "Loss: 2.2373\n",
      "Loss: 1.8518\n",
      "Loss: 2.6210\n",
      "Epoch 3, Loss: 2.2608\n",
      "Loss: 2.2766\n",
      "Loss: 2.2845\n",
      "Loss: 2.5359\n",
      "Loss: 2.6666\n",
      "Loss: 2.4758\n",
      "Loss: 2.9059\n",
      "Loss: 2.5202\n",
      "Loss: 2.5823\n",
      "Loss: 2.3759\n",
      "Loss: 2.3278\n",
      "Loss: 2.3525\n",
      "Loss: 2.9247\n",
      "Loss: 2.4291\n",
      "Loss: 2.3424\n",
      "Loss: 2.4473\n",
      "Loss: 2.1523\n",
      "Loss: 2.4422\n",
      "Loss: 2.5441\n",
      "Loss: 2.2587\n",
      "Loss: 2.2978\n",
      "Loss: 2.2870\n",
      "Loss: 2.2101\n",
      "Loss: 2.1916\n",
      "Loss: 2.2435\n",
      "Loss: 2.4398\n",
      "Loss: 2.3966\n",
      "Loss: 2.4635\n",
      "Loss: 2.3545\n",
      "Loss: 2.0885\n",
      "Loss: 2.5554\n",
      "Loss: 2.0563\n",
      "Loss: 2.3438\n",
      "Loss: 2.6637\n",
      "Loss: 2.6326\n",
      "Loss: 2.0412\n",
      "Loss: 2.6871\n",
      "Loss: 2.4827\n",
      "Loss: 2.4303\n",
      "Loss: 2.4913\n",
      "Loss: 2.0423\n",
      "Loss: 1.9406\n",
      "Loss: 2.4254\n",
      "Loss: 2.9116\n",
      "Loss: 2.5125\n",
      "Loss: 2.2055\n",
      "Loss: 2.1661\n",
      "Loss: 2.4294\n",
      "Loss: 2.2647\n",
      "Loss: 2.6442\n",
      "Loss: 2.3043\n",
      "Loss: 2.3471\n",
      "Loss: 2.9531\n",
      "Loss: 2.2151\n",
      "Loss: 2.4565\n",
      "Loss: 2.0983\n",
      "Loss: 2.4768\n",
      "Loss: 2.3370\n",
      "Loss: 2.6020\n",
      "Loss: 2.2687\n",
      "Loss: 2.2647\n",
      "Loss: 2.4036\n",
      "Loss: 2.5737\n",
      "Loss: 2.4269\n",
      "Loss: 2.3739\n",
      "Loss: 2.4690\n",
      "Loss: 2.0898\n",
      "Loss: 2.5479\n",
      "Loss: 2.4847\n",
      "Loss: 2.1950\n",
      "Loss: 2.5136\n",
      "Loss: 2.6857\n",
      "Loss: 1.7335\n",
      "Loss: 2.5119\n",
      "Loss: 2.2820\n",
      "Loss: 2.1930\n",
      "Loss: 2.6852\n",
      "Loss: 2.4300\n",
      "Loss: 2.2027\n",
      "Loss: 2.2218\n",
      "Loss: 2.3192\n",
      "Loss: 2.4891\n",
      "Loss: 2.4596\n",
      "Loss: 1.8839\n",
      "Loss: 2.0041\n",
      "Loss: 2.4522\n",
      "Loss: 2.3119\n",
      "Loss: 2.3461\n",
      "Loss: 2.8996\n",
      "Loss: 2.4877\n",
      "Loss: 2.5261\n",
      "Loss: 2.3713\n",
      "Loss: 2.5403\n",
      "Loss: 2.2159\n",
      "Loss: 2.0263\n",
      "Loss: 1.8974\n",
      "Loss: 2.3282\n",
      "Loss: 2.4594\n",
      "Loss: 2.5007\n",
      "Loss: 2.1344\n",
      "Loss: 2.5613\n",
      "Loss: 2.3124\n",
      "Loss: 2.5793\n",
      "Loss: 2.6849\n",
      "Loss: 2.0298\n",
      "Loss: 2.5567\n",
      "Loss: 2.0185\n",
      "Loss: 2.7669\n",
      "Loss: 2.8797\n",
      "Loss: 2.5467\n",
      "Loss: 2.3711\n",
      "Loss: 2.3592\n",
      "Loss: 1.9983\n",
      "Loss: 2.1938\n",
      "Loss: 2.4138\n",
      "Loss: 2.4626\n",
      "Loss: 2.3902\n",
      "Loss: 2.4752\n",
      "Loss: 2.5646\n",
      "Loss: 2.5204\n",
      "Loss: 1.4239\n",
      "Loss: 2.2096\n",
      "Loss: 2.3612\n",
      "Loss: 2.1815\n",
      "Loss: 2.3055\n",
      "Loss: 2.2155\n",
      "Loss: 1.9409\n",
      "Loss: 2.0750\n",
      "Loss: 2.3127\n",
      "Loss: 2.8732\n",
      "Loss: 2.2297\n",
      "Loss: 2.3561\n",
      "Loss: 2.5985\n",
      "Loss: 2.7160\n",
      "Loss: 2.2329\n",
      "Loss: 1.9442\n",
      "Loss: 2.6506\n",
      "Loss: 1.8576\n",
      "Epoch 4, Loss: 2.3631\n",
      "Loss: 1.9606\n",
      "Loss: 2.1777\n",
      "Loss: 2.1641\n",
      "Loss: 2.6013\n",
      "Loss: 2.3565\n",
      "Loss: 2.4159\n",
      "Loss: 2.2714\n",
      "Loss: 2.1175\n",
      "Loss: 2.3605\n",
      "Loss: 2.3021\n",
      "Loss: 2.5524\n",
      "Loss: 2.0306\n",
      "Loss: 2.4670\n",
      "Loss: 2.1422\n",
      "Loss: 2.6173\n",
      "Loss: 2.5198\n",
      "Loss: 2.6186\n",
      "Loss: 2.2485\n",
      "Loss: 2.4620\n",
      "Loss: 1.7136\n",
      "Loss: 2.4795\n",
      "Loss: 1.8023\n",
      "Loss: 2.1246\n",
      "Loss: 2.3104\n",
      "Loss: 2.4442\n",
      "Loss: 2.2514\n",
      "Loss: 2.4347\n",
      "Loss: 2.7818\n",
      "Loss: 2.6283\n",
      "Loss: 2.8203\n",
      "Loss: 2.1579\n",
      "Loss: 2.5041\n",
      "Loss: 2.2341\n",
      "Loss: 2.4263\n",
      "Loss: 2.2145\n",
      "Loss: 2.7265\n",
      "Loss: 1.9791\n",
      "Loss: 2.7096\n",
      "Loss: 2.1540\n",
      "Loss: 2.2471\n",
      "Loss: 2.2255\n",
      "Loss: 2.4994\n",
      "Loss: 2.2622\n",
      "Loss: 2.4710\n",
      "Loss: 1.9488\n",
      "Loss: 1.9969\n",
      "Loss: 2.4772\n",
      "Loss: 2.3157\n",
      "Loss: 1.9292\n",
      "Loss: 1.9254\n",
      "Loss: 2.2224\n",
      "Loss: 1.9816\n",
      "Loss: 2.3476\n",
      "Loss: 2.0472\n",
      "Loss: 1.7916\n",
      "Loss: 2.0927\n",
      "Loss: 1.9526\n",
      "Loss: 2.1764\n",
      "Loss: 2.1892\n",
      "Loss: 2.1824\n",
      "Loss: 2.4862\n",
      "Loss: 2.8945\n",
      "Loss: 2.0016\n",
      "Loss: 1.9563\n",
      "Loss: 2.0912\n",
      "Loss: 2.3068\n",
      "Loss: 2.2910\n",
      "Loss: 2.0915\n",
      "Loss: 2.7830\n",
      "Loss: 1.9213\n",
      "Loss: 2.2331\n",
      "Loss: 2.3948\n",
      "Loss: 2.1942\n",
      "Loss: 1.8862\n",
      "Loss: 2.4816\n",
      "Loss: 1.9398\n",
      "Loss: 1.8144\n",
      "Loss: 2.1182\n",
      "Loss: 2.2252\n",
      "Loss: 2.4486\n",
      "Loss: 2.7559\n",
      "Loss: 2.0729\n",
      "Loss: 2.1749\n",
      "Loss: 2.4601\n",
      "Loss: 2.5132\n",
      "Loss: 2.3819\n",
      "Loss: 2.1716\n",
      "Loss: 2.9025\n",
      "Loss: 2.5833\n",
      "Loss: 2.0927\n",
      "Loss: 1.8198\n",
      "Loss: 2.4959\n",
      "Loss: 2.2128\n",
      "Loss: 2.3428\n",
      "Loss: 2.3257\n",
      "Loss: 2.4337\n",
      "Loss: 2.2936\n",
      "Loss: 2.6089\n",
      "Loss: 2.2488\n",
      "Loss: 2.0262\n",
      "Loss: 2.3259\n",
      "Loss: 2.8323\n",
      "Loss: 2.2833\n",
      "Loss: 2.3917\n",
      "Loss: 2.7114\n",
      "Loss: 2.3332\n",
      "Loss: 2.3384\n",
      "Loss: 2.0984\n",
      "Loss: 1.7076\n",
      "Loss: 2.1407\n",
      "Loss: 2.7655\n",
      "Loss: 2.4095\n",
      "Loss: 2.1807\n",
      "Loss: 2.1835\n",
      "Loss: 2.7324\n",
      "Loss: 2.0769\n",
      "Loss: 2.3256\n",
      "Loss: 1.7777\n",
      "Loss: 2.2457\n",
      "Loss: 1.9203\n",
      "Loss: 2.5522\n",
      "Loss: 1.8704\n",
      "Loss: 2.2449\n",
      "Loss: 2.7712\n",
      "Loss: 2.0317\n",
      "Loss: 2.1224\n",
      "Loss: 2.1866\n",
      "Loss: 2.4548\n",
      "Loss: 2.1570\n",
      "Loss: 2.2420\n",
      "Loss: 2.5318\n",
      "Loss: 2.0006\n",
      "Loss: 2.2167\n",
      "Loss: 2.4215\n",
      "Loss: 2.0964\n",
      "Loss: 2.4301\n",
      "Loss: 2.0278\n",
      "Epoch 5, Loss: 3.0917\n",
      "Loss: 2.4652\n",
      "Loss: 2.1613\n",
      "Loss: 2.0643\n",
      "Loss: 1.7916\n",
      "Loss: 2.5070\n",
      "Loss: 2.5471\n",
      "Loss: 2.3349\n",
      "Loss: 2.0971\n",
      "Loss: 2.3906\n",
      "Loss: 1.8070\n",
      "Loss: 2.3823\n",
      "Loss: 2.4850\n",
      "Loss: 1.9577\n",
      "Loss: 2.2252\n",
      "Loss: 1.8715\n",
      "Loss: 2.3503\n",
      "Loss: 2.2409\n",
      "Loss: 2.6208\n",
      "Loss: 1.8529\n",
      "Loss: 2.7073\n",
      "Loss: 1.8639\n",
      "Loss: 2.3884\n",
      "Loss: 1.6978\n",
      "Loss: 1.9058\n",
      "Loss: 2.4461\n",
      "Loss: 1.7133\n",
      "Loss: 2.1428\n",
      "Loss: 2.2925\n",
      "Loss: 2.0869\n",
      "Loss: 1.7397\n",
      "Loss: 2.2816\n",
      "Loss: 1.6571\n",
      "Loss: 2.3098\n",
      "Loss: 2.3794\n",
      "Loss: 1.9575\n",
      "Loss: 1.8517\n",
      "Loss: 2.5219\n",
      "Loss: 2.2524\n",
      "Loss: 2.1071\n",
      "Loss: 2.5428\n",
      "Loss: 2.7837\n",
      "Loss: 2.4923\n",
      "Loss: 1.8568\n",
      "Loss: 2.0528\n",
      "Loss: 1.6595\n",
      "Loss: 1.7274\n",
      "Loss: 2.2210\n",
      "Loss: 1.9263\n",
      "Loss: 2.1634\n",
      "Loss: 2.0528\n",
      "Loss: 2.4932\n",
      "Loss: 2.0802\n",
      "Loss: 2.4585\n",
      "Loss: 2.4880\n",
      "Loss: 2.3640\n",
      "Loss: 2.3484\n",
      "Loss: 1.9382\n",
      "Loss: 2.1314\n",
      "Loss: 1.6304\n",
      "Loss: 1.9675\n",
      "Loss: 2.6440\n",
      "Loss: 2.0796\n",
      "Loss: 2.0913\n",
      "Loss: 2.1326\n",
      "Loss: 2.7214\n",
      "Loss: 2.1126\n",
      "Loss: 1.9087\n",
      "Loss: 2.3038\n",
      "Loss: 2.1503\n",
      "Loss: 1.9100\n",
      "Loss: 1.9482\n",
      "Loss: 2.2798\n",
      "Loss: 2.4508\n",
      "Loss: 2.4394\n",
      "Loss: 2.6494\n",
      "Loss: 1.8406\n",
      "Loss: 2.2538\n",
      "Loss: 1.9811\n",
      "Loss: 2.4947\n",
      "Loss: 2.0061\n",
      "Loss: 1.9399\n",
      "Loss: 2.2605\n",
      "Loss: 1.5340\n",
      "Loss: 1.7116\n",
      "Loss: 1.7074\n",
      "Loss: 2.3951\n",
      "Loss: 2.1976\n",
      "Loss: 2.5486\n",
      "Loss: 2.2162\n",
      "Loss: 2.6812\n",
      "Loss: 2.3906\n",
      "Loss: 2.4088\n",
      "Loss: 2.0546\n",
      "Loss: 2.6518\n",
      "Loss: 2.2460\n",
      "Loss: 2.5012\n",
      "Loss: 1.9581\n",
      "Loss: 2.2810\n",
      "Loss: 2.2869\n",
      "Loss: 2.0729\n",
      "Loss: 2.1391\n",
      "Loss: 2.3419\n",
      "Loss: 1.9229\n",
      "Loss: 2.3383\n",
      "Loss: 2.2074\n",
      "Loss: 2.7628\n",
      "Loss: 2.1577\n",
      "Loss: 2.6293\n",
      "Loss: 2.3779\n",
      "Loss: 2.3021\n",
      "Loss: 1.8657\n",
      "Loss: 2.1050\n",
      "Loss: 1.9233\n",
      "Loss: 1.8784\n",
      "Loss: 1.6973\n",
      "Loss: 2.0355\n",
      "Loss: 2.1168\n",
      "Loss: 2.0851\n",
      "Loss: 2.1728\n",
      "Loss: 2.5329\n",
      "Loss: 2.1993\n",
      "Loss: 2.0577\n",
      "Loss: 2.0077\n",
      "Loss: 2.2509\n",
      "Loss: 2.2359\n",
      "Loss: 2.3221\n",
      "Loss: 2.5585\n",
      "Loss: 2.0242\n",
      "Loss: 1.8717\n",
      "Loss: 2.2553\n",
      "Loss: 1.8912\n",
      "Loss: 1.7841\n",
      "Loss: 1.9250\n",
      "Loss: 1.9737\n",
      "Loss: 2.3674\n",
      "Loss: 1.9300\n",
      "Loss: 2.4521\n",
      "Epoch 6, Loss: 2.4371\n",
      "Loss: 2.1444\n",
      "Loss: 1.6663\n",
      "Loss: 2.0505\n",
      "Loss: 2.3015\n",
      "Loss: 2.8542\n",
      "Loss: 2.1119\n",
      "Loss: 2.3337\n",
      "Loss: 2.0393\n",
      "Loss: 2.1717\n",
      "Loss: 2.1486\n",
      "Loss: 2.5077\n",
      "Loss: 2.1220\n",
      "Loss: 2.1143\n",
      "Loss: 2.6594\n",
      "Loss: 2.2489\n",
      "Loss: 2.0287\n",
      "Loss: 2.1177\n",
      "Loss: 1.7358\n",
      "Loss: 2.3356\n",
      "Loss: 2.2766\n",
      "Loss: 1.8797\n",
      "Loss: 2.8557\n",
      "Loss: 1.8059\n",
      "Loss: 2.6639\n",
      "Loss: 1.7144\n",
      "Loss: 2.2543\n",
      "Loss: 2.3196\n",
      "Loss: 2.3561\n",
      "Loss: 2.0222\n",
      "Loss: 1.8010\n",
      "Loss: 2.3661\n",
      "Loss: 2.0183\n",
      "Loss: 2.1802\n",
      "Loss: 2.3124\n",
      "Loss: 1.4419\n",
      "Loss: 2.2502\n",
      "Loss: 2.0566\n",
      "Loss: 1.6779\n",
      "Loss: 1.8085\n",
      "Loss: 2.3978\n",
      "Loss: 1.6816\n",
      "Loss: 2.7300\n",
      "Loss: 2.2562\n",
      "Loss: 2.1109\n",
      "Loss: 2.2305\n",
      "Loss: 2.8600\n",
      "Loss: 1.8060\n",
      "Loss: 1.8897\n",
      "Loss: 2.1077\n",
      "Loss: 2.3157\n",
      "Loss: 2.0688\n",
      "Loss: 2.4845\n",
      "Loss: 2.0748\n",
      "Loss: 2.4801\n",
      "Loss: 2.0205\n",
      "Loss: 2.3230\n",
      "Loss: 2.3580\n",
      "Loss: 2.2074\n",
      "Loss: 2.3877\n",
      "Loss: 2.3906\n",
      "Loss: 2.4097\n",
      "Loss: 2.1749\n",
      "Loss: 2.3486\n",
      "Loss: 2.0763\n",
      "Loss: 1.7025\n",
      "Loss: 2.1223\n",
      "Loss: 2.2693\n",
      "Loss: 1.9478\n",
      "Loss: 2.6287\n",
      "Loss: 2.1733\n",
      "Loss: 1.9154\n",
      "Loss: 2.7246\n",
      "Loss: 2.6541\n",
      "Loss: 2.3922\n",
      "Loss: 1.8134\n",
      "Loss: 2.1295\n",
      "Loss: 2.0315\n",
      "Loss: 2.1171\n",
      "Loss: 2.1249\n",
      "Loss: 2.1331\n",
      "Loss: 2.1079\n",
      "Loss: 1.7787\n",
      "Loss: 2.0811\n",
      "Loss: 2.0875\n",
      "Loss: 2.3520\n",
      "Loss: 1.9007\n",
      "Loss: 2.4762\n",
      "Loss: 1.7013\n",
      "Loss: 2.0065\n",
      "Loss: 2.0620\n",
      "Loss: 2.6819\n",
      "Loss: 2.3490\n",
      "Loss: 2.0010\n",
      "Loss: 1.9149\n",
      "Loss: 2.0006\n",
      "Loss: 2.0342\n",
      "Loss: 2.1251\n",
      "Loss: 2.2303\n",
      "Loss: 1.6589\n",
      "Loss: 2.9326\n",
      "Loss: 2.2858\n",
      "Loss: 1.1728\n",
      "Loss: 2.3461\n",
      "Loss: 1.9577\n",
      "Loss: 1.7988\n",
      "Loss: 2.2394\n",
      "Loss: 1.7597\n",
      "Loss: 1.6757\n",
      "Loss: 1.9901\n",
      "Loss: 2.3410\n",
      "Loss: 2.3796\n",
      "Loss: 1.9500\n",
      "Loss: 2.5237\n",
      "Loss: 2.1100\n",
      "Loss: 2.0480\n",
      "Loss: 2.4580\n",
      "Loss: 1.9753\n",
      "Loss: 2.2852\n",
      "Loss: 2.1971\n",
      "Loss: 2.5391\n",
      "Loss: 2.1430\n",
      "Loss: 2.3302\n",
      "Loss: 1.9198\n",
      "Loss: 1.9794\n",
      "Loss: 2.0548\n",
      "Loss: 2.2229\n",
      "Loss: 2.4941\n",
      "Loss: 1.9853\n",
      "Loss: 2.1860\n",
      "Loss: 1.6459\n",
      "Loss: 2.4122\n",
      "Loss: 2.2076\n",
      "Loss: 2.3706\n",
      "Loss: 1.7399\n",
      "Loss: 2.2946\n",
      "Loss: 2.5969\n",
      "Loss: 1.9876\n",
      "Epoch 7, Loss: 2.2109\n",
      "Loss: 1.6682\n",
      "Loss: 1.9839\n",
      "Loss: 1.9322\n",
      "Loss: 1.9374\n",
      "Loss: 2.6758\n",
      "Loss: 2.0560\n",
      "Loss: 1.8503\n",
      "Loss: 2.1437\n",
      "Loss: 1.7463\n",
      "Loss: 1.5918\n",
      "Loss: 2.4335\n",
      "Loss: 2.0948\n",
      "Loss: 1.6435\n",
      "Loss: 2.1484\n",
      "Loss: 2.0225\n",
      "Loss: 2.2663\n",
      "Loss: 1.8432\n",
      "Loss: 2.7190\n",
      "Loss: 2.1036\n",
      "Loss: 2.3359\n",
      "Loss: 2.4021\n",
      "Loss: 2.0299\n",
      "Loss: 1.8948\n",
      "Loss: 2.4883\n",
      "Loss: 2.7420\n",
      "Loss: 2.3469\n",
      "Loss: 2.1724\n",
      "Loss: 1.9611\n",
      "Loss: 2.3392\n",
      "Loss: 2.2616\n",
      "Loss: 2.1656\n",
      "Loss: 2.0740\n",
      "Loss: 2.1434\n",
      "Loss: 1.9271\n",
      "Loss: 2.0793\n",
      "Loss: 1.5740\n",
      "Loss: 2.1560\n",
      "Loss: 2.0122\n",
      "Loss: 2.3356\n",
      "Loss: 1.8746\n",
      "Loss: 1.9851\n",
      "Loss: 1.9299\n",
      "Loss: 1.8409\n",
      "Loss: 1.8023\n",
      "Loss: 2.0703\n",
      "Loss: 2.3343\n",
      "Loss: 2.1984\n",
      "Loss: 1.8765\n",
      "Loss: 2.2095\n",
      "Loss: 2.4060\n",
      "Loss: 2.1543\n",
      "Loss: 2.1736\n",
      "Loss: 2.3554\n",
      "Loss: 2.2691\n",
      "Loss: 1.6703\n",
      "Loss: 2.0965\n",
      "Loss: 2.3323\n",
      "Loss: 1.7585\n",
      "Loss: 1.9863\n",
      "Loss: 1.9398\n",
      "Loss: 1.9787\n",
      "Loss: 2.5996\n",
      "Loss: 1.9340\n",
      "Loss: 2.4246\n",
      "Loss: 2.0022\n",
      "Loss: 2.1136\n",
      "Loss: 2.2659\n",
      "Loss: 2.4198\n",
      "Loss: 2.0709\n",
      "Loss: 1.4385\n",
      "Loss: 2.0250\n",
      "Loss: 2.1744\n",
      "Loss: 1.9353\n",
      "Loss: 2.0853\n",
      "Loss: 2.5092\n",
      "Loss: 2.0584\n",
      "Loss: 2.0324\n",
      "Loss: 2.2712\n",
      "Loss: 1.7922\n",
      "Loss: 2.0549\n",
      "Loss: 2.1868\n",
      "Loss: 1.7757\n",
      "Loss: 2.0981\n",
      "Loss: 2.3258\n",
      "Loss: 1.8506\n",
      "Loss: 1.9656\n",
      "Loss: 1.6916\n",
      "Loss: 2.0032\n",
      "Loss: 2.0480\n",
      "Loss: 1.7272\n",
      "Loss: 2.1392\n",
      "Loss: 1.7456\n",
      "Loss: 1.8728\n",
      "Loss: 2.5841\n",
      "Loss: 1.8032\n",
      "Loss: 2.3622\n",
      "Loss: 1.9068\n",
      "Loss: 1.7148\n",
      "Loss: 1.9009\n",
      "Loss: 1.7660\n",
      "Loss: 1.8126\n",
      "Loss: 1.9762\n",
      "Loss: 2.0451\n",
      "Loss: 2.1494\n",
      "Loss: 2.3836\n",
      "Loss: 2.0012\n",
      "Loss: 1.8999\n",
      "Loss: 1.8311\n",
      "Loss: 2.5425\n",
      "Loss: 1.7806\n",
      "Loss: 2.1867\n",
      "Loss: 2.0323\n",
      "Loss: 1.7320\n",
      "Loss: 2.3255\n",
      "Loss: 1.6521\n",
      "Loss: 1.6984\n",
      "Loss: 1.7150\n",
      "Loss: 1.8227\n",
      "Loss: 2.1779\n",
      "Loss: 2.1791\n",
      "Loss: 2.2342\n",
      "Loss: 1.9162\n",
      "Loss: 2.3770\n",
      "Loss: 1.6308\n",
      "Loss: 2.2949\n",
      "Loss: 2.1141\n",
      "Loss: 2.7069\n",
      "Loss: 1.6316\n",
      "Loss: 2.2618\n",
      "Loss: 1.7628\n",
      "Loss: 2.6570\n",
      "Loss: 2.5527\n",
      "Loss: 2.2857\n",
      "Loss: 2.1683\n",
      "Loss: 1.9895\n",
      "Loss: 1.7652\n",
      "Epoch 8, Loss: 1.9024\n",
      "Loss: 1.9182\n",
      "Loss: 1.9058\n",
      "Loss: 2.0249\n",
      "Loss: 2.0564\n",
      "Loss: 2.0699\n",
      "Loss: 2.1947\n",
      "Loss: 1.9630\n",
      "Loss: 1.6357\n",
      "Loss: 2.0195\n",
      "Loss: 2.3421\n",
      "Loss: 2.0748\n",
      "Loss: 1.5407\n",
      "Loss: 2.3787\n",
      "Loss: 1.9816\n",
      "Loss: 1.4323\n",
      "Loss: 2.6010\n",
      "Loss: 2.3113\n",
      "Loss: 2.3739\n",
      "Loss: 1.8998\n",
      "Loss: 2.0963\n",
      "Loss: 1.6702\n",
      "Loss: 1.7941\n",
      "Loss: 2.0449\n",
      "Loss: 1.9602\n",
      "Loss: 2.0738\n",
      "Loss: 2.0534\n",
      "Loss: 1.7457\n",
      "Loss: 2.0318\n",
      "Loss: 1.8938\n",
      "Loss: 2.0152\n",
      "Loss: 1.6798\n",
      "Loss: 1.9261\n",
      "Loss: 2.1769\n",
      "Loss: 2.1610\n",
      "Loss: 1.8367\n",
      "Loss: 1.8264\n",
      "Loss: 2.4740\n",
      "Loss: 2.0039\n",
      "Loss: 2.3617\n",
      "Loss: 1.8001\n",
      "Loss: 2.7210\n",
      "Loss: 2.5537\n",
      "Loss: 1.9035\n",
      "Loss: 2.1678\n",
      "Loss: 2.0673\n",
      "Loss: 2.0022\n",
      "Loss: 1.6840\n",
      "Loss: 1.7691\n",
      "Loss: 1.8161\n",
      "Loss: 2.4213\n",
      "Loss: 1.6212\n",
      "Loss: 1.9871\n",
      "Loss: 2.4470\n",
      "Loss: 1.8770\n",
      "Loss: 1.7691\n",
      "Loss: 1.8843\n",
      "Loss: 1.9882\n",
      "Loss: 1.5353\n",
      "Loss: 1.6922\n",
      "Loss: 2.0421\n",
      "Loss: 2.2662\n",
      "Loss: 1.6017\n",
      "Loss: 2.1301\n",
      "Loss: 1.8955\n",
      "Loss: 2.0813\n",
      "Loss: 2.2318\n",
      "Loss: 1.8669\n",
      "Loss: 1.5382\n",
      "Loss: 2.1798\n",
      "Loss: 1.7925\n",
      "Loss: 1.7985\n",
      "Loss: 1.9631\n",
      "Loss: 1.6388\n",
      "Loss: 2.3913\n",
      "Loss: 1.8907\n",
      "Loss: 2.2942\n",
      "Loss: 2.2458\n",
      "Loss: 1.8126\n",
      "Loss: 1.8712\n",
      "Loss: 2.2291\n",
      "Loss: 1.9690\n",
      "Loss: 2.3205\n",
      "Loss: 1.5177\n",
      "Loss: 1.6996\n",
      "Loss: 1.7399\n",
      "Loss: 2.3035\n",
      "Loss: 2.2955\n",
      "Loss: 2.4719\n",
      "Loss: 2.0673\n",
      "Loss: 1.9938\n",
      "Loss: 1.9601\n",
      "Loss: 2.1364\n",
      "Loss: 2.1923\n",
      "Loss: 2.1223\n",
      "Loss: 2.0225\n",
      "Loss: 1.7064\n",
      "Loss: 1.6281\n",
      "Loss: 1.9555\n",
      "Loss: 1.8608\n",
      "Loss: 1.9711\n",
      "Loss: 1.3445\n",
      "Loss: 2.2802\n",
      "Loss: 1.5992\n",
      "Loss: 1.5142\n",
      "Loss: 1.9344\n",
      "Loss: 2.0410\n",
      "Loss: 2.0677\n",
      "Loss: 1.9219\n",
      "Loss: 2.3057\n",
      "Loss: 2.2915\n",
      "Loss: 2.6941\n",
      "Loss: 2.1632\n",
      "Loss: 1.6667\n",
      "Loss: 2.6025\n",
      "Loss: 2.1674\n",
      "Loss: 2.4822\n",
      "Loss: 1.9450\n",
      "Loss: 2.1664\n",
      "Loss: 1.9658\n",
      "Loss: 2.2629\n",
      "Loss: 1.5201\n",
      "Loss: 1.5971\n",
      "Loss: 2.2654\n",
      "Loss: 1.6799\n",
      "Loss: 1.7127\n",
      "Loss: 1.9064\n",
      "Loss: 1.8044\n",
      "Loss: 1.8423\n",
      "Loss: 2.1133\n",
      "Loss: 2.1610\n",
      "Loss: 1.9695\n",
      "Loss: 2.1460\n",
      "Loss: 2.0009\n",
      "Loss: 1.9212\n",
      "Loss: 2.1618\n",
      "Loss: 2.4747\n",
      "Loss: 1.8441\n",
      "Epoch 9, Loss: 2.0304\n",
      "Loss: 2.3394\n",
      "Loss: 1.8901\n",
      "Loss: 1.9909\n",
      "Loss: 2.3726\n",
      "Loss: 1.8516\n",
      "Loss: 2.1561\n",
      "Loss: 1.5562\n",
      "Loss: 1.7014\n",
      "Loss: 2.3997\n",
      "Loss: 1.9819\n",
      "Loss: 1.9586\n",
      "Loss: 2.0113\n",
      "Loss: 2.0011\n",
      "Loss: 2.2338\n",
      "Loss: 1.9342\n",
      "Loss: 2.2026\n",
      "Loss: 1.6909\n",
      "Loss: 2.2892\n",
      "Loss: 1.4510\n",
      "Loss: 2.4264\n",
      "Loss: 2.2958\n",
      "Loss: 1.9409\n",
      "Loss: 2.3357\n",
      "Loss: 1.7132\n",
      "Loss: 1.9065\n",
      "Loss: 2.8827\n",
      "Loss: 2.4068\n",
      "Loss: 1.7088\n",
      "Loss: 1.7079\n",
      "Loss: 1.7286\n",
      "Loss: 2.1439\n",
      "Loss: 1.9331\n",
      "Loss: 2.1346\n",
      "Loss: 1.6965\n",
      "Loss: 1.8420\n",
      "Loss: 2.3331\n",
      "Loss: 2.3981\n",
      "Loss: 1.6280\n",
      "Loss: 1.8964\n",
      "Loss: 1.7786\n",
      "Loss: 1.6606\n",
      "Loss: 1.7189\n",
      "Loss: 2.2465\n",
      "Loss: 2.2138\n",
      "Loss: 1.6680\n",
      "Loss: 2.2561\n",
      "Loss: 2.1529\n",
      "Loss: 1.9910\n",
      "Loss: 1.7073\n",
      "Loss: 1.6274\n",
      "Loss: 1.9002\n",
      "Loss: 2.2748\n",
      "Loss: 1.8480\n",
      "Loss: 2.1385\n",
      "Loss: 1.8344\n",
      "Loss: 1.9248\n",
      "Loss: 2.2599\n",
      "Loss: 1.7992\n",
      "Loss: 1.9655\n",
      "Loss: 1.7787\n",
      "Loss: 1.8223\n",
      "Loss: 1.8066\n",
      "Loss: 1.9033\n",
      "Loss: 2.4548\n",
      "Loss: 2.5734\n",
      "Loss: 2.0155\n",
      "Loss: 2.0348\n",
      "Loss: 2.0890\n",
      "Loss: 1.8750\n",
      "Loss: 2.0334\n",
      "Loss: 1.8198\n",
      "Loss: 2.0224\n",
      "Loss: 1.7097\n",
      "Loss: 1.7813\n",
      "Loss: 2.4793\n",
      "Loss: 1.8706\n",
      "Loss: 1.5154\n",
      "Loss: 1.6886\n",
      "Loss: 2.0359\n",
      "Loss: 2.3373\n",
      "Loss: 1.4133\n",
      "Loss: 1.9008\n",
      "Loss: 1.2431\n",
      "Loss: 1.5740\n",
      "Loss: 2.4279\n",
      "Loss: 1.6362\n",
      "Loss: 1.7987\n",
      "Loss: 1.9890\n",
      "Loss: 2.0113\n",
      "Loss: 1.9126\n",
      "Loss: 1.6701\n",
      "Loss: 1.7259\n",
      "Loss: 1.9492\n",
      "Loss: 1.7023\n",
      "Loss: 1.9459\n",
      "Loss: 1.8312\n",
      "Loss: 1.9651\n",
      "Loss: 1.7791\n",
      "Loss: 2.1769\n",
      "Loss: 1.3934\n",
      "Loss: 1.8227\n",
      "Loss: 2.0054\n",
      "Loss: 2.4971\n",
      "Loss: 1.6764\n",
      "Loss: 1.6161\n",
      "Loss: 2.0003\n",
      "Loss: 1.8025\n",
      "Loss: 1.6911\n",
      "Loss: 1.9855\n",
      "Loss: 2.1219\n",
      "Loss: 2.0976\n",
      "Loss: 1.7634\n",
      "Loss: 2.0196\n",
      "Loss: 2.1314\n",
      "Loss: 2.0480\n",
      "Loss: 2.0913\n",
      "Loss: 1.8457\n",
      "Loss: 1.3542\n",
      "Loss: 2.0293\n",
      "Loss: 1.9475\n",
      "Loss: 1.9346\n",
      "Loss: 1.8836\n",
      "Loss: 1.8403\n",
      "Loss: 2.0837\n",
      "Loss: 2.2766\n",
      "Loss: 2.7500\n",
      "Loss: 1.7643\n",
      "Loss: 2.3518\n",
      "Loss: 2.1415\n",
      "Loss: 2.3470\n",
      "Loss: 1.9396\n",
      "Loss: 1.7762\n",
      "Loss: 2.3562\n",
      "Loss: 2.0421\n",
      "Loss: 2.1406\n",
      "Loss: 1.7572\n",
      "Loss: 1.9536\n",
      "Epoch 10, Loss: 1.9893\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(char_to_idx) + 1\n",
    "embed_dim = 12\n",
    "hidden_dim = 128\n",
    "num_layers = 4\n",
    "\n",
    "# Initialize model\n",
    "model = CharLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "count = 0\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # (batch_size, vocab_size)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 188])\n",
      "Inputs: nd have already executed a Waiver of Rights and endorsement for the lifting of the Notice of Acquisi\n",
      "Predicted: ['t', 'c', 'n']\n",
      "Targets: t\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in dataloader:\n",
    "    outputs = model(inputs)\n",
    "    print(outputs.shape)\n",
    "    top3_preds = torch.topk(outputs, 3, dim=1)[1]\n",
    "    translated_input = [idx_to_char[idx.item()] for idx in inputs[0]]\n",
    "    translated_output = [idx_to_char[idx.item()] for idx in top3_preds[0]]\n",
    "    translated_target = idx_to_char[targets[0].item()]\n",
    "\n",
    "    print(\"Inputs:\", ''.join(translated_input))\n",
    "    print(\"Predicted:\", translated_output)\n",
    "    print(\"Targets:\", translated_target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse447",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
