{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/envs/cse447/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.nn import LSTM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"uonlp/CulturaX\", \"en\", split='train', streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_head = ds.take(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(example):\n",
    "    text = example[\"text\"]\n",
    "    pairs = []\n",
    "    for i in range(1, len(text)):\n",
    "        inputs = text[:i]\n",
    "        outputs = text[i]\n",
    "        if len(inputs) > MAX_LENGTH:\n",
    "            inputs = inputs[-MAX_LENGTH:]\n",
    "        pairs.append({\"input\": inputs, \"output\": outputs})\n",
    "    return pairs\n",
    "data_pairs = []\n",
    "for d in ds_head:\n",
    "    data_pairs.extend(create_pairs(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "1864434\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "num_chars = 0\n",
    "for d in ds_head:\n",
    "    num_chars += len(d['text'])\n",
    "    chars.update(d['text'])\n",
    "print(len(chars))\n",
    "print(num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    return [char_to_idx[char] for char in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "outputs = []\n",
    "for d in data_pairs:\n",
    "    sequences.append(torch.tensor(encode_sequence(d[\"input\"]), dtype=torch.long))\n",
    "    outputs.append(torch.tensor(char_to_idx[d[\"output\"]], dtype=torch.long))\n",
    "\n",
    "X = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "y = torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1863934, 50])\n",
      "tensor([39, 50, 55,  3, 36, 81,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "torch.Size([1863934])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[5])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'X': X, 'y': y,\n",
    "    'char_to_idx': char_to_idx, 'idx_to_char': idx_to_char\n",
    "}, 'dataset_splits.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1863934, 50])\n",
      "torch.Size([1863934])\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '#': 6, '$': 7, '%': 8, '&': 9, \"'\": 10, '(': 11, ')': 12, '*': 13, '+': 14, ',': 15, '-': 16, '.': 17, '/': 18, '0': 19, '1': 20, '2': 21, '3': 22, '4': 23, '5': 24, '6': 25, '7': 26, '8': 27, '9': 28, ':': 29, ';': 30, '<': 31, '=': 32, '>': 33, '?': 34, '@': 35, 'A': 36, 'B': 37, 'C': 38, 'D': 39, 'E': 40, 'F': 41, 'G': 42, 'H': 43, 'I': 44, 'J': 45, 'K': 46, 'L': 47, 'M': 48, 'N': 49, 'O': 50, 'P': 51, 'Q': 52, 'R': 53, 'S': 54, 'T': 55, 'U': 56, 'V': 57, 'W': 58, 'X': 59, 'Y': 60, 'Z': 61, '[': 62, '\\\\': 63, ']': 64, '^': 65, '_': 66, '`': 67, 'a': 68, 'b': 69, 'c': 70, 'd': 71, 'e': 72, 'f': 73, 'g': 74, 'h': 75, 'i': 76, 'j': 77, 'k': 78, 'l': 79, 'm': 80, 'n': 81, 'o': 82, 'p': 83, 'q': 84, 'r': 85, 's': 86, 't': 87, 'u': 88, 'v': 89, 'w': 90, 'x': 91, 'y': 92, 'z': 93, '{': 94, '|': 95, '}': 96, '~': 97, '\\x9d': 98, '\\xa0': 99, '£': 100, '§': 101, '©': 102, '«': 103, '®': 104, '°': 105, '±': 106, '²': 107, '´': 108, '·': 109, 'º': 110, '»': 111, '¼': 112, '½': 113, 'Ç': 114, 'Ó': 115, '×': 116, 'à': 117, 'á': 118, 'â': 119, 'ã': 120, 'ä': 121, 'æ': 122, 'ç': 123, 'è': 124, 'é': 125, 'ê': 126, 'ë': 127, 'ì': 128, 'í': 129, 'î': 130, 'ï': 131, 'ð': 132, 'ñ': 133, 'ó': 134, 'ô': 135, 'õ': 136, 'ö': 137, 'ø': 138, 'ù': 139, 'ú': 140, 'ü': 141, 'ā': 142, 'ă': 143, 'Č': 144, 'č': 145, 'Đ': 146, 'đ': 147, 'Ğ': 148, 'ğ': 149, 'ģ': 150, 'ı': 151, 'ř': 152, 'Ş': 153, 'ş': 154, 'š': 155, 'ž': 156, 'ư': 157, 'ʻ': 158, 'Δ': 159, 'Ε': 160, 'ά': 161, 'η': 162, 'ι': 163, 'κ': 164, 'λ': 165, 'μ': 166, 'ν': 167, 'М': 168, 'Р': 169, 'С': 170, 'Ф': 171, 'а': 172, 'в': 173, 'д': 174, 'е': 175, 'и': 176, 'к': 177, 'л': 178, 'м': 179, 'н': 180, 'о': 181, 'п': 182, 'р': 183, 'с': 184, 'т': 185, 'ч': 186, 'я': 187, 'ё': 188, 'ا': 189, 'ب': 190, 'ة': 191, 'ر': 192, 'ع': 193, 'ل': 194, 'ي': 195, 'ạ': 196, 'ả': 197, 'ấ': 198, 'ầ': 199, 'ậ': 200, 'ắ': 201, 'ẽ': 202, 'ế': 203, 'ề': 204, 'ể': 205, 'ị': 206, 'ọ': 207, 'ổ': 208, 'ớ': 209, 'ờ': 210, 'ợ': 211, 'ủ': 212, 'ứ': 213, 'ử': 214, 'ự': 215, 'ỳ': 216, '\\u2009': 217, '\\u200b': 218, '\\u200e': 219, '‐': 220, '–': 221, '—': 222, '‘': 223, '’': 224, '“': 225, '”': 226, '•': 227, '…': 228, '″': 229, '›': 230, '€': 231, '₱': 232, '™': 233, '←': 234, '↑': 235, '→': 236, '−': 237, '①': 238, '②': 239, '③': 240, '④': 241, '⑤': 242, '┐': 243, '╫': 244, '▀': 245, '■': 246, '▷': 247, '★': 248, '✅': 249, '⟨': 250, '⟩': 251, '【': 252, '】': 253, '一': 254, '中': 255, '体': 256, '作': 257, '力': 258, '发': 259, '听': 260, '回': 261, '布': 262, '慢': 263, '摘': 264, '文': 265, '日': 266, '期': 267, '本': 268, '洒': 269, '潇': 270, '简': 271, '者': 272, '英': 273, '要': 274, '語': 275, '语': 276, '走': 277, '速': 278, '\\uf04a': 279, '：': 280, '�': 281, '𝐴': 282, '𝑅': 283, '🇦': 284, '🇨': 285, '🎶': 286, '👍': 287, '😆': 288, '😉': 289, '😊': 290, '😕': 291, '🙂': 292, '🤗': 293}\n",
      "{1: '\\t', 2: '\\n', 3: ' ', 4: '!', 5: '\"', 6: '#', 7: '$', 8: '%', 9: '&', 10: \"'\", 11: '(', 12: ')', 13: '*', 14: '+', 15: ',', 16: '-', 17: '.', 18: '/', 19: '0', 20: '1', 21: '2', 22: '3', 23: '4', 24: '5', 25: '6', 26: '7', 27: '8', 28: '9', 29: ':', 30: ';', 31: '<', 32: '=', 33: '>', 34: '?', 35: '@', 36: 'A', 37: 'B', 38: 'C', 39: 'D', 40: 'E', 41: 'F', 42: 'G', 43: 'H', 44: 'I', 45: 'J', 46: 'K', 47: 'L', 48: 'M', 49: 'N', 50: 'O', 51: 'P', 52: 'Q', 53: 'R', 54: 'S', 55: 'T', 56: 'U', 57: 'V', 58: 'W', 59: 'X', 60: 'Y', 61: 'Z', 62: '[', 63: '\\\\', 64: ']', 65: '^', 66: '_', 67: '`', 68: 'a', 69: 'b', 70: 'c', 71: 'd', 72: 'e', 73: 'f', 74: 'g', 75: 'h', 76: 'i', 77: 'j', 78: 'k', 79: 'l', 80: 'm', 81: 'n', 82: 'o', 83: 'p', 84: 'q', 85: 'r', 86: 's', 87: 't', 88: 'u', 89: 'v', 90: 'w', 91: 'x', 92: 'y', 93: 'z', 94: '{', 95: '|', 96: '}', 97: '~', 98: '\\x9d', 99: '\\xa0', 100: '£', 101: '§', 102: '©', 103: '«', 104: '®', 105: '°', 106: '±', 107: '²', 108: '´', 109: '·', 110: 'º', 111: '»', 112: '¼', 113: '½', 114: 'Ç', 115: 'Ó', 116: '×', 117: 'à', 118: 'á', 119: 'â', 120: 'ã', 121: 'ä', 122: 'æ', 123: 'ç', 124: 'è', 125: 'é', 126: 'ê', 127: 'ë', 128: 'ì', 129: 'í', 130: 'î', 131: 'ï', 132: 'ð', 133: 'ñ', 134: 'ó', 135: 'ô', 136: 'õ', 137: 'ö', 138: 'ø', 139: 'ù', 140: 'ú', 141: 'ü', 142: 'ā', 143: 'ă', 144: 'Č', 145: 'č', 146: 'Đ', 147: 'đ', 148: 'Ğ', 149: 'ğ', 150: 'ģ', 151: 'ı', 152: 'ř', 153: 'Ş', 154: 'ş', 155: 'š', 156: 'ž', 157: 'ư', 158: 'ʻ', 159: 'Δ', 160: 'Ε', 161: 'ά', 162: 'η', 163: 'ι', 164: 'κ', 165: 'λ', 166: 'μ', 167: 'ν', 168: 'М', 169: 'Р', 170: 'С', 171: 'Ф', 172: 'а', 173: 'в', 174: 'д', 175: 'е', 176: 'и', 177: 'к', 178: 'л', 179: 'м', 180: 'н', 181: 'о', 182: 'п', 183: 'р', 184: 'с', 185: 'т', 186: 'ч', 187: 'я', 188: 'ё', 189: 'ا', 190: 'ب', 191: 'ة', 192: 'ر', 193: 'ع', 194: 'ل', 195: 'ي', 196: 'ạ', 197: 'ả', 198: 'ấ', 199: 'ầ', 200: 'ậ', 201: 'ắ', 202: 'ẽ', 203: 'ế', 204: 'ề', 205: 'ể', 206: 'ị', 207: 'ọ', 208: 'ổ', 209: 'ớ', 210: 'ờ', 211: 'ợ', 212: 'ủ', 213: 'ứ', 214: 'ử', 215: 'ự', 216: 'ỳ', 217: '\\u2009', 218: '\\u200b', 219: '\\u200e', 220: '‐', 221: '–', 222: '—', 223: '‘', 224: '’', 225: '“', 226: '”', 227: '•', 228: '…', 229: '″', 230: '›', 231: '€', 232: '₱', 233: '™', 234: '←', 235: '↑', 236: '→', 237: '−', 238: '①', 239: '②', 240: '③', 241: '④', 242: '⑤', 243: '┐', 244: '╫', 245: '▀', 246: '■', 247: '▷', 248: '★', 249: '✅', 250: '⟨', 251: '⟩', 252: '【', 253: '】', 254: '一', 255: '中', 256: '体', 257: '作', 258: '力', 259: '发', 260: '听', 261: '回', 262: '布', 263: '慢', 264: '摘', 265: '文', 266: '日', 267: '期', 268: '本', 269: '洒', 270: '潇', 271: '简', 272: '者', 273: '英', 274: '要', 275: '語', 276: '语', 277: '走', 278: '速', 279: '\\uf04a', 280: '：', 281: '�', 282: '𝐴', 283: '𝑅', 284: '🇦', 285: '🇨', 286: '🎶', 287: '👍', 288: '😆', 289: '😉', 290: '😊', 291: '😕', 292: '🙂', 293: '🤗'}\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('dataset_splits.pth')\n",
    "X, y = data['X'], data['y']\n",
    "char_to_idx = data['char_to_idx']\n",
    "idx_to_char = data['idx_to_char']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, outputs, device):\n",
    "        self.sequences = sequences.to(device)\n",
    "        self.outputs = outputs.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.outputs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharDataset(train_set[:][0], train_set[:][1], device)\n",
    "val_dataset = CharDataset(val_set[:][0], val_set[:][1], device)\n",
    "test_dataset = CharDataset(test_set[:][0], test_set[:][1], device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Output size is vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last LSTM output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.5096, Train Acc: 0.3184, Val Loss: 2.1357, Val Acc: 0.3986\n",
      "Epoch 2: Train Loss: 1.9921, Train Acc: 0.4370, Val Loss: 1.8841, Val Acc: 0.4662\n",
      "Epoch 3: Train Loss: 1.8026, Train Acc: 0.4891, Val Loss: 1.7489, Val Acc: 0.5037\n",
      "Epoch 4: Train Loss: 1.6870, Train Acc: 0.5196, Val Loss: 1.6685, Val Acc: 0.5251\n",
      "Epoch 5: Train Loss: 1.6094, Train Acc: 0.5391, Val Loss: 1.6179, Val Acc: 0.5385\n",
      "Epoch 6: Train Loss: 1.5525, Train Acc: 0.5531, Val Loss: 1.5846, Val Acc: 0.5467\n",
      "Epoch 7: Train Loss: 1.5071, Train Acc: 0.5646, Val Loss: 1.5628, Val Acc: 0.5524\n",
      "Epoch 8: Train Loss: 1.4687, Train Acc: 0.5745, Val Loss: 1.5479, Val Acc: 0.5569\n",
      "Epoch 9: Train Loss: 1.4346, Train Acc: 0.5832, Val Loss: 1.5386, Val Acc: 0.5597\n",
      "Epoch 10: Train Loss: 1.4035, Train Acc: 0.5912, Val Loss: 1.5339, Val Acc: 0.5614\n",
      "Epoch 11: Train Loss: 1.3745, Train Acc: 0.5989, Val Loss: 1.5321, Val Acc: 0.5629\n",
      "Epoch 12: Train Loss: 1.3470, Train Acc: 0.6063, Val Loss: 1.5326, Val Acc: 0.5634\n",
      "Epoch 13: Train Loss: 1.3209, Train Acc: 0.6135, Val Loss: 1.5367, Val Acc: 0.5637\n",
      "Epoch 14: Train Loss: 1.2956, Train Acc: 0.6209, Val Loss: 1.5421, Val Acc: 0.5634\n",
      "Epoch 15: Train Loss: 1.2892, Train Acc: 0.6216, Val Loss: 1.5205, Val Acc: 0.5690\n",
      "Epoch 16: Train Loss: nan, Train Acc: 0.0779, Val Loss: nan, Val Acc: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cse447/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define search space\n",
    "lr = 1e-4\n",
    "embed_dim = 12\n",
    "hidden_dim = 256\n",
    "num_layers = 4\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(char_to_idx) + 1\n",
    "model = CharLSTM(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_correct, train_total, running_loss = 0, 0, 0\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "        train_total += targets.numel()\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct, val_total, val_loss = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.numel()\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss\n",
    "    }\n",
    "    torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=4, lr=0.0005\n",
    "# Epoch 1: Train Loss: 2.6495, Train Acc: 0.2909, Val Loss: 2.2317, Val Acc: 0.3762\n",
    "# Epoch 2: Train Loss: 2.1025, Train Acc: 0.4079, Val Loss: 2.0086, Val Acc: 0.4310\n",
    "# Epoch 3: Train Loss: 1.9364, Train Acc: 0.4512, Val Loss: 1.8976, Val Acc: 0.4602\n",
    "# Epoch 4: Train Loss: 1.8342, Train Acc: 0.4787, Val Loss: 1.8251, Val Acc: 0.4817\n",
    "# Epoch 5: Train Loss: 1.7652, Train Acc: 0.4972, Val Loss: 1.7746, Val Acc: 0.4959\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=4, lr=0.0001\n",
    "# Epoch 1: Train Loss: 3.0272, Train Acc: 0.2024, Val Loss: 2.6667, Val Acc: 0.2684\n",
    "# Epoch 2: Train Loss: 2.4985, Train Acc: 0.3109, Val Loss: 2.3811, Val Acc: 0.3391\n",
    "# Epoch 3: Train Loss: 2.2985, Train Acc: 0.3593, Val Loss: 2.2286, Val Acc: 0.3745\n",
    "# Epoch 4: Train Loss: 2.1686, Train Acc: 0.3902, Val Loss: 2.1256, Val Acc: 0.4000\n",
    "# Epoch 5: Train Loss: 2.0770, Train Acc: 0.4131, Val Loss: 2.0493, Val Acc: 0.4199\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=6, lr=0.0005\n",
    "# Epoch 1: Train Loss: 3.2215, Train Acc: 0.1623, Val Loss: 3.2171, Val Acc: 0.1615\n",
    "# Epoch 2: Train Loss: 3.2188, Train Acc: 0.1623, Val Loss: 3.2173, Val Acc: 0.1615\n",
    "# Epoch 3: Train Loss: 3.2188, Train Acc: 0.1623, Val Loss: 3.2174, Val Acc: 0.1615\n",
    "# Epoch 4: Train Loss: 3.2187, Train Acc: 0.1623, Val Loss: 3.2173, Val Acc: 0.1615\n",
    "# Epoch 5: Train Loss: 3.2187, Train Acc: 0.1623, Val Loss: 3.2173, Val Acc: 0.1615\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=128, num_layers=6, lr=0.0001\n",
    "# Epoch 1: Train Loss: 3.1103, Train Acc: 0.1844, Val Loss: 2.7249, Val Acc: 0.2656\n",
    "# Epoch 2: Train Loss: 2.5756, Train Acc: 0.2931, Val Loss: 2.4823, Val Acc: 0.3131\n",
    "# Epoch 3: Train Loss: 2.4225, Train Acc: 0.3272, Val Loss: 2.3761, Val Acc: 0.3368\n",
    "# Epoch 4: Train Loss: 2.3367, Train Acc: 0.3435, Val Loss: 2.3057, Val Acc: 0.3497\n",
    "# Epoch 5: Train Loss: 2.2668, Train Acc: 0.3586, Val Loss: 2.2428, Val Acc: 0.3663\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=256, num_layers=4, lr=0.0005\n",
    "# Epoch 1: Train Loss: 2.6740, Train Acc: 0.2834, Val Loss: 2.1641, Val Acc: 0.3929\n",
    "# Epoch 2: Train Loss: 2.0015, Train Acc: 0.4329, Val Loss: 1.8835, Val Acc: 0.4626\n",
    "# Epoch 3: Train Loss: 1.7981, Train Acc: 0.4868, Val Loss: 1.7642, Val Acc: 0.4960\n",
    "# Epoch 4: Train Loss: 1.6872, Train Acc: 0.5156, Val Loss: 1.7059, Val Acc: 0.5112\n",
    "# Epoch 5: Train Loss: 1.6139, Train Acc: 0.5344, Val Loss: 1.6718, Val Acc: 0.5219\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=256, num_layers=4, lr=0.0001\n",
    "# Epoch 1: Train Loss: 2.9654, Train Acc: 0.2245, Val Loss: 2.6269, Val Acc: 0.2990\n",
    "# Epoch 2: Train Loss: 2.5003, Train Acc: 0.3223, Val Loss: 2.3906, Val Acc: 0.3427\n",
    "# Epoch 3: Train Loss: 2.3068, Train Acc: 0.3618, Val Loss: 2.2278, Val Acc: 0.3792\n",
    "# Epoch 4: Train Loss: 2.1653, Train Acc: 0.3951, Val Loss: 2.1111, Val Acc: 0.4095\n",
    "# Epoch 5: Train Loss: 2.0632, Train Acc: 0.4199, Val Loss: 2.0294, Val Acc: 0.4275\n",
    "\n",
    "# Training with: embed_dim=8, hidden_dim=256, num_layers=6, lr=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7174\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for inputs, targets in test_dataloader:\n",
    "    outputs = model(inputs)\n",
    "    top3_preds = torch.topk(outputs, 3, dim=1)[1]\n",
    "    translated_output = set([idx_to_char[str(idx.item())] for idx in top3_preds[0]])\n",
    "    translated_target = idx_to_char[str(targets[0].item())]\n",
    "    if translated_target in translated_output:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    # print(\"Inputs:\", ''.join(translated_input))\n",
    "    # print(\"Predicted:\", translated_output)\n",
    "    # print(\"Targets:\", translated_target)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '\\t', '2': '\\n', '3': ' ', '4': '!', '5': '\"', '6': '#', '7': '$', '8': '%', '9': '&', '10': \"'\", '11': '(', '12': ')', '13': '*', '14': '+', '15': ',', '16': '-', '17': '.', '18': '/', '19': '0', '20': '1', '21': '2', '22': '3', '23': '4', '24': '5', '25': '6', '26': '7', '27': '8', '28': '9', '29': ':', '30': ';', '31': '<', '32': '=', '33': '>', '34': '?', '35': '@', '36': 'A', '37': 'B', '38': 'C', '39': 'D', '40': 'E', '41': 'F', '42': 'G', '43': 'H', '44': 'I', '45': 'J', '46': 'K', '47': 'L', '48': 'M', '49': 'N', '50': 'O', '51': 'P', '52': 'Q', '53': 'R', '54': 'S', '55': 'T', '56': 'U', '57': 'V', '58': 'W', '59': 'X', '60': 'Y', '61': 'Z', '62': '[', '63': '\\\\', '64': ']', '65': '^', '66': '_', '67': '`', '68': 'a', '69': 'b', '70': 'c', '71': 'd', '72': 'e', '73': 'f', '74': 'g', '75': 'h', '76': 'i', '77': 'j', '78': 'k', '79': 'l', '80': 'm', '81': 'n', '82': 'o', '83': 'p', '84': 'q', '85': 'r', '86': 's', '87': 't', '88': 'u', '89': 'v', '90': 'w', '91': 'x', '92': 'y', '93': 'z', '94': '{', '95': '|', '96': '}', '97': '~', '98': '\\x9d', '99': '\\xa0', '100': '£', '101': '§', '102': '©', '103': '«', '104': '\\xad', '105': '®', '106': '°', '107': '±', '108': '²', '109': '´', '110': 'µ', '111': '·', '112': 'º', '113': '»', '114': '¼', '115': '½', '116': '¾', '117': 'Á', '118': 'Â', '119': 'Ç', '120': 'Ó', '121': 'Ö', '122': '×', '123': 'à', '124': 'á', '125': 'â', '126': 'ã', '127': 'ä', '128': 'æ', '129': 'ç', '130': 'è', '131': 'é', '132': 'ê', '133': 'ë', '134': 'ì', '135': 'í', '136': 'î', '137': 'ï', '138': 'ð', '139': 'ñ', '140': 'ò', '141': 'ó', '142': 'ô', '143': 'õ', '144': 'ö', '145': 'ø', '146': 'ù', '147': 'ú', '148': 'ü', '149': 'ý', '150': 'ā', '151': 'ă', '152': 'Č', '153': 'č', '154': 'Đ', '155': 'đ', '156': 'ě', '157': 'Ğ', '158': 'ğ', '159': 'ģ', '160': 'ĩ', '161': 'ı', '162': 'ř', '163': 'ś', '164': 'Ş', '165': 'ş', '166': 'š', '167': 'ž', '168': 'ơ', '169': 'ư', '170': 'ʻ', '171': 'Δ', '172': 'Ε', '173': 'ά', '174': 'α', '175': 'β', '176': 'η', '177': 'ι', '178': 'κ', '179': 'λ', '180': 'μ', '181': 'ν', '182': 'χ', '183': 'Б', '184': 'Ж', '185': 'М', '186': 'П', '187': 'Р', '188': 'С', '189': 'Ф', '190': 'а', '191': 'в', '192': 'д', '193': 'е', '194': 'и', '195': 'й', '196': 'к', '197': 'л', '198': 'м', '199': 'н', '200': 'о', '201': 'п', '202': 'р', '203': 'с', '204': 'т', '205': 'ч', '206': 'ш', '207': 'ь', '208': 'я', '209': 'ё', '210': 'ا', '211': 'ب', '212': 'ة', '213': 'ر', '214': 'ع', '215': 'ل', '216': 'ي', '217': 'ค', '218': 'ต', '219': 'ท', '220': 'บ', '221': 'ป', '222': 'ย', '223': 'ร', '224': 'ศ', '225': 'ส', '226': 'ะ', '227': 'า', '228': 'ู', '229': 'เ', '230': 'ไ', '231': '่', '232': 'ạ', '233': 'ả', '234': 'ấ', '235': 'ầ', '236': 'ẫ', '237': 'ậ', '238': 'ắ', '239': 'ằ', '240': 'ẳ', '241': 'ặ', '242': 'ẹ', '243': 'ẽ', '244': 'ế', '245': 'ề', '246': 'ể', '247': 'ễ', '248': 'ệ', '249': 'ỉ', '250': 'ị', '251': 'ọ', '252': 'ỏ', '253': 'ố', '254': 'ồ', '255': 'ổ', '256': 'ỗ', '257': 'ộ', '258': 'ớ', '259': 'ờ', '260': 'ở', '261': 'ợ', '262': 'ụ', '263': 'ủ', '264': 'ứ', '265': 'ừ', '266': 'ử', '267': 'ữ', '268': 'ự', '269': 'ỳ', '270': '\\u2008', '271': '\\u2009', '272': '\\u200b', '273': '\\u200e', '274': '‐', '275': '–', '276': '—', '277': '‘', '278': '’', '279': '“', '280': '”', '281': '„', '282': '†', '283': '•', '284': '…', '285': '′', '286': '″', '287': '›', '288': '€', '289': '₱', '290': '₹', '291': '℗', '292': '℘', '293': '™', '294': '←', '295': '↑', '296': '→', '297': '↓', '298': '⇐', '299': '⇒', '300': '−', '301': '≈', '302': '≤', '303': '≥', '304': '①', '305': '②', '306': '③', '307': '④', '308': '⑤', '309': '┐', '310': '╫', '311': '▀', '312': '■', '313': '▷', '314': '►', '315': '◦', '316': '★', '317': '☴', '318': '♕', '319': '✅', '320': '⟨', '321': '⟩', '322': '【', '323': '】', '324': '一', '325': '中', '326': '体', '327': '作', '328': '力', '329': '发', '330': '听', '331': '回', '332': '布', '333': '慢', '334': '摘', '335': '文', '336': '新', '337': '无', '338': '日', '339': '最', '340': '期', '341': '本', '342': '洒', '343': '清', '344': '潇', '345': '码', '346': '简', '347': '繁', '348': '者', '349': '英', '350': '要', '351': '語', '352': '语', '353': '走', '354': '速', '355': '道', '356': '體', '357': '高', '358': '국', '359': '노', '360': '라', '361': '바', '362': '사', '363': '이', '364': '지', '365': '카', '366': '트', '367': '한', '368': '\\uf04a', '369': '：', '370': '�', '371': '𝐴', '372': '𝑅', '373': '🇦', '374': '🇨', '375': '🎶', '376': '👍', '377': '😀', '378': '😁', '379': '😆', '380': '😉', '381': '😊', '382': '😕', '383': '😥', '384': '🙂', '385': '🤔', '386': '🤗', '387': '🤣'}\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse447",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
